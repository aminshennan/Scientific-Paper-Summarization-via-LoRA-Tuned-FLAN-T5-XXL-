# Scientificâ€‘Paperâ€‘Summarizer

A full endâ€‘toâ€‘end pipeline that harvests scientific papers from arXiv, cleans and explores the data, fineâ€‘tunes **FLANâ€‘T5â€‘XXL** with **LoRA**, evaluates the improved model, and serves realâ€‘time abstractive summaries through a Flask web UI.

> **Highlight:** +2.6 ROUGEâ€‘1 and +1.1 ROUGEâ€‘2 over the base model on a heldâ€‘out arXiv test set while using <1% of the original parameters for training.

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/) 
[![License: Academic](https://img.shields.io/badge/License-Academic-yellow.svg)](#license)

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Features](#features)
3. [Repository Structure](#repository-structure)
4. [Quick Start](#quick-start)
5. [Reproducing the Results](#reproducing-the-results)
6. [Running the Web Demo](#running-the-web-demo)
7. [Report & Presentation](#report--presentation)
8. [Architecture](#architecture)
9. [Results](#results)
10. [Contributing](#contributing)
11. [License](#license)
12. [References](#references)

---

## Project Overview

This Finalâ€‘Year Project (FYP) explores parameterâ€‘efficient fineâ€‘tuning for longâ€‘form scientific summarisation.

* **Data** â€“ >200k arXiv papers harvested via the official API, filtered to relevant computer science categories.
* **Model** â€“ LoRAâ€‘adapted *FLANâ€‘T5â€‘XXL* (11B parameters, rank = 16, Î± = 32) trained with 8â€‘bit quantization for memory efficiency.
* **Evaluation** â€“ ROUGE metrics + qualitative comparisons on held-out test set.
* **Deployment** â€“ Flask + Bootstrap dark theme UI with drag-and-drop functionality and ngrok tunneling.

---

## Features

* **Modular Jupyter notebooks** covering data acquisition, EDA, training, evaluation and hosting.
* **Parameter-efficient training** using LoRA (Low-Rank Adaptation) for fine-tuning large models.
* **Mixedâ€‘precision + 8â€‘bit loading** to fit XXL models in commodity VRAM.
* **Automated evaluation suite** to benchmark trained LoRA checkpoints.
* **Interactive web interface** with drag-and-drop support for TXT, PDF, and DOCX files.
* **Ngrok integration** for quick remote demos and sharing.

---

## Repository Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 1 data featching.ipynb     # Harvest arXiv metadata & full-text
â”‚   â”œâ”€â”€ 2 final EDA.ipynb          # Cleaning, token stats, exploratory analysis
â”‚   â”œâ”€â”€ 3 Model training.ipynb     # LoRA fineâ€‘tuning + checkpoint saving
â”‚   â”œâ”€â”€ 4 Testing.ipynb            # ROUGE evaluation suite & model comparison
â”‚   â”œâ”€â”€ 5 Hosting.ipynb            # Flask app development + ngrok tunnel setup
â”‚   â”œâ”€â”€ index.html                 # Standâ€‘alone dark theme UI
â”‚   â”œâ”€â”€ LoRA/                      # Saved LoRA adapter weights (post-training)
â”‚   â””â”€â”€ data/                      # Raw & processed datasets (*.csv files - not in Git)
â”‚       â”œâ”€â”€ cs_papers_api.csv      # ğŸš« Gitignored (237 MB)
â”‚       â”œâ”€â”€ remaining_papers.csv   # ğŸš« Gitignored (329 MB) 
â”‚       â””â”€â”€ final_combined_dataset.csv # ğŸš« Gitignored (565 MB)
â”œâ”€â”€ Final Report.pdf               # ğŸ“„ Complete technical report
â”œâ”€â”€ Poster.pdf                     # ğŸ¨ Academic conference poster
â”œâ”€â”€ Slides.pptx                    # ğŸ“Š Presentation slides
â””â”€â”€ README.md                      # ğŸ“– This file
```

> **âš ï¸ Important:** Large CSV datasets (`src/data/*.csv`) are **not tracked in Git** due to GitHub's 100MB file size limit. These files are generated by running `src/1 data featching.ipynb` and are automatically ignored via `.gitignore`. The total dataset size is ~1.1GB.

---

## Quick Start

**Clone and set up environment:**

```bash
git clone https://github.com/aminshennan/Scientific-Paper-Summarization-via-LoRA-Tuned-FLAN-T5-XXL-.git
cd Scientific-Paper-Summarization-via-LoRA-Tuned-FLAN-T5-XXL-
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

**Install dependencies:**
The notebooks contain their own `!pip install` commands for specific requirements. Key libraries include:
```bash
pip install pandas requests pdfminer.six PyMuPDF transformers datasets accelerate evaluate peft bitsandbytes loralib jupyterlab flask pyngrok rouge-score tensorboard py7zr
```

**Launch Jupyter Lab:**
```bash
jupyter lab
# Execute notebooks in src/ in numerical order (1 through 5)
```

---

## Reproducing the Results

1. **Data Collection:** Run `src/1 data featching.ipynb` to download and process arXiv papers (~200k samples).
2. **Exploratory Analysis:** Execute `src/2 final EDA.ipynb` for data cleaning and statistics.
3. **Model Training:** Launch `src/3 Model training.ipynb` to fineâ€‘tune FLAN-T5-XXL with LoRA (requires substantial GPU memory).
4. **Evaluation:** Run `src/4 Testing.ipynb` to compute ROUGE metrics and generate comparisons.

**Expected Results:** 
- ROUGE-1: ~31.20 (vs. 28.58 baseline)
- ROUGE-2: ~8.62 (vs. 7.48 baseline)  
- ROUGE-L: ~17.56 (vs. 16.83 baseline)

---

## Running the Web Demo

1. **Start the Flask backend:**
   ```bash
   # Execute src/5 Hosting.ipynb completely, or extract the Flask code to app.py
   python app.py  # Flask server typically runs on port 5000
   ```

2. **Access the UI:**
   - Open `src/index.html` directly in your browser, OR
   - Navigate to `http://localhost:5000` if Flask serves static files
   - For public access: Use the ngrok tunnel URL generated in the hosting notebook

3. **Usage:**
   - Drag & drop TXT/PDF/DOCX files onto the interface
   - Click "Summarize" to generate abstractive summaries
   - Copy results or download as TXT file


---

## Report & Presentation

* **`Final Report.pdf`** â€“ Comprehensive methodology, experiments, results, and discussion.
* **`Poster.pdf`** â€“ Academic conference poster summarizing key contributions.
* **`Slides.pptx`** â€“ Final presentation slides for project defense.

These documents provide detailed insights into the research methodology, experimental setup, and findings.

---

## Architecture

The project follows a modular pipeline:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   arXiv     â”‚    â”‚    Data      â”‚    â”‚    LoRA     â”‚    â”‚  Web UI &    â”‚
â”‚ API Harvest â”œâ”€â”€â–º â”‚  Cleaning &  â”œâ”€â”€â–º â”‚ Fine-tuning â”œâ”€â”€â–º â”‚  Evaluation  â”‚
â”‚             â”‚    â”‚     EDA      â”‚    â”‚             â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚                   â”‚
        â–¼                   â–¼                   â–¼                   â–¼
   Raw Papers         Processed Data      LoRA Weights        Flask Demo
```

**Key Components:**
- **Data Pipeline:** Automated arXiv harvesting and preprocessing
- **Model Training:** Parameter-efficient LoRA adaptation of FLAN-T5-XXL  
- **Evaluation:** Quantitative (ROUGE) and qualitative assessment
- **Deployment:** Interactive web interface with real-time summarization

---

## Results

Evaluation on held-out test set (as per `src/4 Testing.ipynb`):

| Metric     | Base FLAN-T5-XXL | LoRA-tuned FLAN-T5-XXL | Î” (Improvement) |
|:-----------|:-----------------|:-----------------------|:----------------|
| ROUGE-1    | 28.58            | **31.20**              | +2.62           |
| ROUGE-2    | 7.48             | **8.62**               | +1.14           |
| ROUGE-L    | 16.83            | **17.56**              | +0.73           |
| ROUGE-Lsum | 16.76            | **17.49**              | +0.73           |

**Key Findings:**
- LoRA fine-tuning achieved significant improvements across all ROUGE metrics
- Training used <1% of original model parameters (rank=16, Î±=32)
- 8-bit quantization enabled training on consumer GPU hardware
- Generated summaries show improved coherence and domain-specific terminology

---

## Contributing

1. **Fork** the repository and create a feature branch (`git checkout -b feature/awesome-improvement`)
2. **Commit** your changes with descriptive messages
3. **Push** to your branch and open a Pull Request
4. Describe your changes, motivation, and any testing performed

**Development Guidelines:**
- Follow existing code structure and naming conventions
- Test changes with provided evaluation notebooks
- Update documentation for any new features

---

## License

This project is provided **for academic and research purposes only**. Please cite appropriately if you build upon this work.

**Citation:**
```bibtex
@misc{scientific-paper-summarizer-2024,
  title={Scientific Paper Summarization via LoRA-Tuned FLAN-T5-XXL},
  author={[Your Name]},
  year={2024},
  howpublished={Final Year Project, Multimedia University},
  url={https://github.com/aminshennan/Scientific-Paper-Summarization-via-LoRA-Tuned-FLAN-T5-XXL-}
}
```

---

## References

* Hu et al., "LoRA: Lowâ€‘Rank Adaptation of Large Language Models" (2022)
* Chung et al., "Scaling Instruction-Finetuned Language Models" (2022) - FLAN-T5
* Lin, "ROUGE: A Package for Automatic Evaluation of Summaries" (2004)
* arXiv API Documentation â€“ [https://arxiv.org/help/api/](https://arxiv.org/help/api/)
* Hugging Face Transformers & PEFT Libraries

---

> **Questions or Collaboration?** Open an [Issue](https://github.com/aminshennan/Scientific-Paper-Summarization-via-LoRA-Tuned-FLAN-T5-XXL-/issues) or contact via email: [aminshennan@gmail.com](mailto:aminshennan@gmail.com) 