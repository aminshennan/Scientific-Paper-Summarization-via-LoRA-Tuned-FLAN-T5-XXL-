# Scientific‑Paper‑Summarizer

A full end‑to‑end pipeline that harvests scientific papers from arXiv, cleans and explores the data, fine‑tunes **FLAN‑T5‑XXL** with **LoRA**, evaluates the improved model, and serves real‑time abstractive summaries through a Flask web UI.

> **Highlight:** +2.6 ROUGE‑1 and +1.1 ROUGE‑2 over the base model on a held‑out arXiv test set while using <1% of the original parameters for training.

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/) 
[![License: Academic](https://img.shields.io/badge/License-Academic-yellow.svg)](#license)

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Features](#features)
3. [Repository Structure](#repository-structure)
4. [Quick Start](#quick-start)
5. [Reproducing the Results](#reproducing-the-results)
6. [Running the Web Demo](#running-the-web-demo)
7. [Report & Presentation](#report--presentation)
8. [Architecture](#architecture)
9. [Results](#results)
10. [Contributing](#contributing)
11. [License](#license)
12. [References](#references)

---

## Project Overview

This Final‑Year Project (FYP) explores parameter‑efficient fine‑tuning for long‑form scientific summarisation.

* **Data** – >200k arXiv papers harvested via the official API, filtered to relevant computer science categories.
* **Model** – LoRA‑adapted *FLAN‑T5‑XXL* (11B parameters, rank = 16, α = 32) trained with 8‑bit quantization for memory efficiency.
* **Evaluation** – ROUGE metrics + qualitative comparisons on held-out test set.
* **Deployment** – Flask + Bootstrap dark theme UI with drag-and-drop functionality and ngrok tunneling.

---

## Features

* **Modular Jupyter notebooks** covering data acquisition, EDA, training, evaluation and hosting.
* **Parameter-efficient training** using LoRA (Low-Rank Adaptation) for fine-tuning large models.
* **Mixed‑precision + 8‑bit loading** to fit XXL models in commodity VRAM.
* **Automated evaluation suite** to benchmark trained LoRA checkpoints.
* **Interactive web interface** with drag-and-drop support for TXT, PDF, and DOCX files.
* **Ngrok integration** for quick remote demos and sharing.

---

## Repository Structure

```
├── src/
│   ├── 1 data featching.ipynb     # Harvest arXiv metadata & full-text
│   ├── 2 final EDA.ipynb          # Cleaning, token stats, exploratory analysis
│   ├── 3 Model training.ipynb     # LoRA fine‑tuning + checkpoint saving
│   ├── 4 Testing.ipynb            # ROUGE evaluation suite & model comparison
│   ├── 5 Hosting.ipynb            # Flask app development + ngrok tunnel setup
│   ├── index.html                 # Stand‑alone dark theme UI
│   ├── LoRA/                      # Saved LoRA adapter weights (post-training)
│   └── data/                      # Raw & processed datasets (*.csv files - not in Git)
│       ├── cs_papers_api.csv      # 🚫 Gitignored (237 MB)
│       ├── remaining_papers.csv   # 🚫 Gitignored (329 MB) 
│       └── final_combined_dataset.csv # 🚫 Gitignored (565 MB)
├── Final Report.pdf               # 📄 Complete technical report
├── Poster.pdf                     # 🎨 Academic conference poster
├── Slides.pptx                    # 📊 Presentation slides
└── README.md                      # 📖 This file
```

> **⚠️ Important:** Large CSV datasets (`src/data/*.csv`) are **not tracked in Git** due to GitHub's 100MB file size limit. These files are generated by running `src/1 data featching.ipynb` and are automatically ignored via `.gitignore`. The total dataset size is ~1.1GB.

---

## Quick Start

**Clone and set up environment:**

```bash
git clone https://github.com/aminshennan/Scientific-Paper-Summarization-via-LoRA-Tuned-FLAN-T5-XXL-.git
cd Scientific-Paper-Summarization-via-LoRA-Tuned-FLAN-T5-XXL-
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

**Install dependencies:**
The notebooks contain their own `!pip install` commands for specific requirements. Key libraries include:
```bash
pip install pandas requests pdfminer.six PyMuPDF transformers datasets accelerate evaluate peft bitsandbytes loralib jupyterlab flask pyngrok rouge-score tensorboard py7zr
```

**Launch Jupyter Lab:**
```bash
jupyter lab
# Execute notebooks in src/ in numerical order (1 through 5)
```

---

## Reproducing the Results

1. **Data Collection:** Run `src/1 data featching.ipynb` to download and process arXiv papers (~200k samples).
2. **Exploratory Analysis:** Execute `src/2 final EDA.ipynb` for data cleaning and statistics.
3. **Model Training:** Launch `src/3 Model training.ipynb` to fine‑tune FLAN-T5-XXL with LoRA (requires substantial GPU memory).
4. **Evaluation:** Run `src/4 Testing.ipynb` to compute ROUGE metrics and generate comparisons.

**Expected Results:** 
- ROUGE-1: ~31.20 (vs. 28.58 baseline)
- ROUGE-2: ~8.62 (vs. 7.48 baseline)  
- ROUGE-L: ~17.56 (vs. 16.83 baseline)

---

## Running the Web Demo

1. **Start the Flask backend:**
   ```bash
   # Execute src/5 Hosting.ipynb completely, or extract the Flask code to app.py
   python app.py  # Flask server typically runs on port 5000
   ```

2. **Access the UI:**
   - Open `src/index.html` directly in your browser, OR
   - Navigate to `http://localhost:5000` if Flask serves static files
   - For public access: Use the ngrok tunnel URL generated in the hosting notebook

3. **Usage:**
   - Drag & drop TXT/PDF/DOCX files onto the interface
   - Click "Summarize" to generate abstractive summaries
   - Copy results or download as TXT file


---

## Report & Presentation

* **`Final Report.pdf`** – Comprehensive methodology, experiments, results, and discussion.
* **`Poster.pdf`** – Academic conference poster summarizing key contributions.
* **`Slides.pptx`** – Final presentation slides for project defense.

These documents provide detailed insights into the research methodology, experimental setup, and findings.

---

## Architecture

The project follows a modular pipeline:

```text
┌─────────────┐    ┌──────────────┐    ┌─────────────┐    ┌──────────────┐
│   arXiv     │    │    Data      │    │    LoRA     │    │  Web UI &    │
│ API Harvest ├──► │  Cleaning &  ├──► │ Fine-tuning ├──► │  Evaluation  │
│             │    │     EDA      │    │             │    │              │
└─────────────┘    └──────────────┘    └─────────────┘    └──────────────┘
        │                   │                   │                   │
        ▼                   ▼                   ▼                   ▼
   Raw Papers         Processed Data      LoRA Weights        Flask Demo
```

**Key Components:**
- **Data Pipeline:** Automated arXiv harvesting and preprocessing
- **Model Training:** Parameter-efficient LoRA adaptation of FLAN-T5-XXL  
- **Evaluation:** Quantitative (ROUGE) and qualitative assessment
- **Deployment:** Interactive web interface with real-time summarization

---

## Results

Evaluation on held-out test set (as per `src/4 Testing.ipynb`):

| Metric     | Base FLAN-T5-XXL | LoRA-tuned FLAN-T5-XXL | Δ (Improvement) |
|:-----------|:-----------------|:-----------------------|:----------------|
| ROUGE-1    | 28.58            | **31.20**              | +2.62           |
| ROUGE-2    | 7.48             | **8.62**               | +1.14           |
| ROUGE-L    | 16.83            | **17.56**              | +0.73           |
| ROUGE-Lsum | 16.76            | **17.49**              | +0.73           |

**Key Findings:**
- LoRA fine-tuning achieved significant improvements across all ROUGE metrics
- Training used <1% of original model parameters (rank=16, α=32)
- 8-bit quantization enabled training on consumer GPU hardware
- Generated summaries show improved coherence and domain-specific terminology

---

## Contributing

1. **Fork** the repository and create a feature branch (`git checkout -b feature/awesome-improvement`)
2. **Commit** your changes with descriptive messages
3. **Push** to your branch and open a Pull Request
4. Describe your changes, motivation, and any testing performed

**Development Guidelines:**
- Follow existing code structure and naming conventions
- Test changes with provided evaluation notebooks
- Update documentation for any new features

---

## License

This project is provided **for academic and research purposes only**. Please cite appropriately if you build upon this work.

**Citation:**
```bibtex
@misc{scientific-paper-summarizer-2024,
  title={Scientific Paper Summarization via LoRA-Tuned FLAN-T5-XXL},
  author={[Your Name]},
  year={2024},
  howpublished={Final Year Project, Multimedia University},
  url={https://github.com/aminshennan/Scientific-Paper-Summarization-via-LoRA-Tuned-FLAN-T5-XXL-}
}
```

---

## References

* Hu et al., "LoRA: Low‑Rank Adaptation of Large Language Models" (2022)
* Chung et al., "Scaling Instruction-Finetuned Language Models" (2022) - FLAN-T5
* Lin, "ROUGE: A Package for Automatic Evaluation of Summaries" (2004)
* arXiv API Documentation – [https://arxiv.org/help/api/](https://arxiv.org/help/api/)
* Hugging Face Transformers & PEFT Libraries

---

> **Questions or Collaboration?** Open an [Issue](https://github.com/aminshennan/Scientific-Paper-Summarization-via-LoRA-Tuned-FLAN-T5-XXL-/issues) or contact via email: [aminshennan@gmail.com](mailto:aminshennan@gmail.com) 