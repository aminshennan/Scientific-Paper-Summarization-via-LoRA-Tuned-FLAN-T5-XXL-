{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f07744b",
      "metadata": {
        "id": "2f07744b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import os\n",
        "from pdfminer.high_level import extract_text\n",
        "import time\n",
        "from pdfminer.psparser import PSSyntaxError\n",
        "import fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa577b4a",
      "metadata": {
        "id": "aa577b4a"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "cs_papers = pd.read_csv(\"data/cs_papers_api.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29c779fd",
      "metadata": {
        "id": "29c779fd"
      },
      "outputs": [],
      "source": [
        "category_mapping = {\n",
        "    'cs.CV': 'Computer Vision and Pattern Recognition',\n",
        "    'cs.NE': 'Neural and Evolutionary Computing',\n",
        "    'cs.MA': 'Multiagent Systems',\n",
        "    'cs.RO': 'Robotics',\n",
        "    'cs.CL': 'Computation and Language',\n",
        "    'cs.LG': 'Machine Learning',\n",
        "    'cs.AI': 'Artificial Intelligence',\n",
        "    'cs.CR': 'Cryptography and Security',\n",
        "    'cs.HC': 'Human-Computer Interaction',\n",
        "    'cs.IR': 'Information Retrieval',\n",
        "    'cs.GT': 'Computer Science and Game Theory',\n",
        "    'cs.SE': 'Software Engineering',\n",
        "    'cs.CY': 'Computers and Society',\n",
        "    'cs.ET': 'Emerging Technologies',\n",
        "    'cs.NI': 'Networking and Internet Architecture',\n",
        "    'cs.MM': 'Multimedia',\n",
        "    'cs.SI': 'Social and Information Networks',\n",
        "    'cs.CC': 'Computational Complexity',\n",
        "    'cs.DB': 'Databases',\n",
        "    'cs.IT': 'Information Theory',\n",
        "    'cs.PL': 'Programming Languages',\n",
        "    'cs.DS': 'Data Structures and Algorithms',\n",
        "    'cs.SD': 'Sound',\n",
        "    'cs.LO': 'Logic in Computer Science',\n",
        "    'cs.DL': 'Digital Libraries',\n",
        "    'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
        "    'cs.OH': 'Other Computer Science',\n",
        "    'cs.CE': 'Computational Engineering, Finance, and Science',\n",
        "    'cs.AR': 'Hardware Architecture',\n",
        "    'cs.FL': 'Formal Languages and Automata Theory',\n",
        "    'cs.GR': 'Graphics',\n",
        "    'cs.MS': 'Mathematical Software',\n",
        "    'cs.CG': 'Computational Geometry',\n",
        "    'cs.SC': 'Symbolic Computation',\n",
        "    'cs.PF': 'Performance',\n",
        "    'cs.OS': 'Operating Systems',\n",
        "    'cs.DM': 'Discrete Mathematics',\n",
        "    'cs.NA': 'Numerical Analysis',\n",
        "    'cs.SY': 'Systems and Control',\n",
        "    'cs.GL': 'General Literature'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1408ba4d",
      "metadata": {
        "id": "1408ba4d"
      },
      "outputs": [],
      "source": [
        "# Map the primary_category column to full titles\n",
        "cs_papers['primary_category_full'] = cs_papers['primary_category'].map(category_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f355fd6f",
      "metadata": {
        "id": "f355fd6f",
        "outputId": "129293c1-2b6d-4acf-f41a-f81b3c1c91f7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>year</th>\n",
              "      <th>primary_category</th>\n",
              "      <th>categories</th>\n",
              "      <th>primary_category_full</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2301.02657v1</td>\n",
              "      <td>TarViS: A Unified Approach for Target-based Vi...</td>\n",
              "      <td>The general domain of video segmentation is cu...</td>\n",
              "      <td>2023-01-06 18:59:52+00:00</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>cs.CV cs.AI cs.LG</td>\n",
              "      <td>Computer Vision and Pattern Recognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2301.02642v1</td>\n",
              "      <td>Triple-stream Deep Metric Learning of Great Ap...</td>\n",
              "      <td>We propose the first metric learning system fo...</td>\n",
              "      <td>2023-01-06 18:36:04+00:00</td>\n",
              "      <td>cs.CV</td>\n",
              "      <td>cs.CV cs.AI cs.LG</td>\n",
              "      <td>Computer Vision and Pattern Recognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2301.02610v1</td>\n",
              "      <td>Feedback-Gated Rectified Linear Units</td>\n",
              "      <td>Feedback connections play a prominent role in ...</td>\n",
              "      <td>2023-01-06 17:14:11+00:00</td>\n",
              "      <td>cs.NE</td>\n",
              "      <td>cs.NE cs.AI</td>\n",
              "      <td>Neural and Evolutionary Computing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2301.02593v1</td>\n",
              "      <td>Multi-Agent Reinforcement Learning for Fast-Ti...</td>\n",
              "      <td>To integrate high amounts of renewable energy ...</td>\n",
              "      <td>2023-01-06 16:41:51+00:00</td>\n",
              "      <td>cs.MA</td>\n",
              "      <td>cs.MA cs.AI cs.LG cs.SY</td>\n",
              "      <td>Multiagent Systems</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2301.02561v1</td>\n",
              "      <td>Multi-Vehicle Trajectory Prediction at Interse...</td>\n",
              "      <td>Traditional approaches to prediction of future...</td>\n",
              "      <td>2023-01-06 15:13:23+00:00</td>\n",
              "      <td>cs.RO</td>\n",
              "      <td>cs.RO cs.AI</td>\n",
              "      <td>Robotics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200089</th>\n",
              "      <td>1610.07090v1</td>\n",
              "      <td>STEPS: Predicting place attributes via spatio-...</td>\n",
              "      <td>In recent years, a vast amount of research has...</td>\n",
              "      <td>2016-10-22 19:41:44+00:00</td>\n",
              "      <td>cs.SI</td>\n",
              "      <td>cs.SI</td>\n",
              "      <td>Social and Information Networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200090</th>\n",
              "      <td>1610.07772v1</td>\n",
              "      <td>Visual Themes and Sentiment on Social Networks...</td>\n",
              "      <td>Online Social Networks explode with activity w...</td>\n",
              "      <td>2016-10-25 07:56:43+00:00</td>\n",
              "      <td>cs.SI</td>\n",
              "      <td>cs.SI</td>\n",
              "      <td>Social and Information Networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200091</th>\n",
              "      <td>1610.08098v2</td>\n",
              "      <td>The Effect of Pokémon Go on The Pulse of the C...</td>\n",
              "      <td>Pok\\'emon Go, a location-based game that uses ...</td>\n",
              "      <td>2017-09-18 16:50:12+00:00</td>\n",
              "      <td>cs.SI</td>\n",
              "      <td>cs.SI</td>\n",
              "      <td>Social and Information Networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200092</th>\n",
              "      <td>1610.08686v1</td>\n",
              "      <td>Polarized User and Topic Tracking in Twitter</td>\n",
              "      <td>Digital traces of conversations in micro-blogg...</td>\n",
              "      <td>2016-10-27 10:03:31+00:00</td>\n",
              "      <td>cs.SI</td>\n",
              "      <td>cs.SI</td>\n",
              "      <td>Social and Information Networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200093</th>\n",
              "      <td>1610.09002v1</td>\n",
              "      <td>The Effect of Pets on Happiness: A Data-Driven...</td>\n",
              "      <td>Psychologists have demonstrated that pets have...</td>\n",
              "      <td>2016-10-27 20:24:26+00:00</td>\n",
              "      <td>cs.SI</td>\n",
              "      <td>cs.SI</td>\n",
              "      <td>Social and Information Networks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200094 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            paper_id                                              title  \\\n",
              "0       2301.02657v1  TarViS: A Unified Approach for Target-based Vi...   \n",
              "1       2301.02642v1  Triple-stream Deep Metric Learning of Great Ap...   \n",
              "2       2301.02610v1              Feedback-Gated Rectified Linear Units   \n",
              "3       2301.02593v1  Multi-Agent Reinforcement Learning for Fast-Ti...   \n",
              "4       2301.02561v1  Multi-Vehicle Trajectory Prediction at Interse...   \n",
              "...              ...                                                ...   \n",
              "200089  1610.07090v1  STEPS: Predicting place attributes via spatio-...   \n",
              "200090  1610.07772v1  Visual Themes and Sentiment on Social Networks...   \n",
              "200091  1610.08098v2  The Effect of Pokémon Go on The Pulse of the C...   \n",
              "200092  1610.08686v1       Polarized User and Topic Tracking in Twitter   \n",
              "200093  1610.09002v1  The Effect of Pets on Happiness: A Data-Driven...   \n",
              "\n",
              "                                                 abstract  \\\n",
              "0       The general domain of video segmentation is cu...   \n",
              "1       We propose the first metric learning system fo...   \n",
              "2       Feedback connections play a prominent role in ...   \n",
              "3       To integrate high amounts of renewable energy ...   \n",
              "4       Traditional approaches to prediction of future...   \n",
              "...                                                   ...   \n",
              "200089  In recent years, a vast amount of research has...   \n",
              "200090  Online Social Networks explode with activity w...   \n",
              "200091  Pok\\'emon Go, a location-based game that uses ...   \n",
              "200092  Digital traces of conversations in micro-blogg...   \n",
              "200093  Psychologists have demonstrated that pets have...   \n",
              "\n",
              "                             year primary_category               categories  \\\n",
              "0       2023-01-06 18:59:52+00:00            cs.CV        cs.CV cs.AI cs.LG   \n",
              "1       2023-01-06 18:36:04+00:00            cs.CV        cs.CV cs.AI cs.LG   \n",
              "2       2023-01-06 17:14:11+00:00            cs.NE              cs.NE cs.AI   \n",
              "3       2023-01-06 16:41:51+00:00            cs.MA  cs.MA cs.AI cs.LG cs.SY   \n",
              "4       2023-01-06 15:13:23+00:00            cs.RO              cs.RO cs.AI   \n",
              "...                           ...              ...                      ...   \n",
              "200089  2016-10-22 19:41:44+00:00            cs.SI                    cs.SI   \n",
              "200090  2016-10-25 07:56:43+00:00            cs.SI                    cs.SI   \n",
              "200091  2017-09-18 16:50:12+00:00            cs.SI                    cs.SI   \n",
              "200092  2016-10-27 10:03:31+00:00            cs.SI                    cs.SI   \n",
              "200093  2016-10-27 20:24:26+00:00            cs.SI                    cs.SI   \n",
              "\n",
              "                          primary_category_full  \n",
              "0       Computer Vision and Pattern Recognition  \n",
              "1       Computer Vision and Pattern Recognition  \n",
              "2             Neural and Evolutionary Computing  \n",
              "3                            Multiagent Systems  \n",
              "4                                      Robotics  \n",
              "...                                         ...  \n",
              "200089          Social and Information Networks  \n",
              "200090          Social and Information Networks  \n",
              "200091          Social and Information Networks  \n",
              "200092          Social and Information Networks  \n",
              "200093          Social and Information Networks  \n",
              "\n",
              "[200094 rows x 7 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cs_papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88553cb3",
      "metadata": {
        "id": "88553cb3"
      },
      "outputs": [],
      "source": [
        "def fetch_metadata(paper_id, retries=5, delay=10):\n",
        "    base_url = f\"http://export.arxiv.org/api/query?id_list={paper_id}\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(base_url, timeout=20)\n",
        "            if response.status_code == 200:\n",
        "                return response.text\n",
        "            elif response.status_code == 500:\n",
        "                print(f\"HTTP 500 error for paper_id {paper_id}, retrying...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"Failed to fetch metadata for paper_id {paper_id}, Status code: {response.status_code}\")\n",
        "                return None\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"Failed to fetch metadata for paper_id {paper_id} after {retries} attempts.\")\n",
        "                return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3163e9cd",
      "metadata": {
        "id": "3163e9cd"
      },
      "outputs": [],
      "source": [
        "def parse_pdf_url(response_text):\n",
        "    root = ET.fromstring(response_text)\n",
        "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
        "        for link in entry.findall('{http://www.w3.org/2005/Atom}link'):\n",
        "            if link.attrib.get('title') == 'pdf':\n",
        "                return link.attrib['href']\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b00f7e4",
      "metadata": {
        "id": "2b00f7e4"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace with a single space\n",
        "    text = re.sub(r'\\[[^]]*\\]', '', text)  # Remove text in square brackets\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)  # Remove text in parentheses\n",
        "    text = re.sub(r'<[^>]*>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)  # Remove URLs\n",
        "    text = text.strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2c6611",
      "metadata": {
        "id": "fd2c6611"
      },
      "outputs": [],
      "source": [
        "def remove_abstract_from_full_text(full_text, abstract):\n",
        "    clean_abstract = clean_text(abstract)\n",
        "    abstract_pattern = re.escape(clean_abstract)\n",
        "    full_text_cleaned = re.sub(abstract_pattern, '', full_text, flags=re.IGNORECASE)\n",
        "\n",
        "    if full_text_cleaned == full_text:\n",
        "        keywords = [\"introduction\", \"1 introduction\", \"background\", \"related work\", \"methodology\", \"methods\",\n",
        "                    \"results\", \"discussion\", \" Keywords\"]\n",
        "        abstract_start = re.search(r'\\babstract\\b', full_text, re.IGNORECASE)\n",
        "        if not abstract_start:\n",
        "            return full_text\n",
        "\n",
        "        abstract_end = len(full_text)\n",
        "        for keyword in keywords:\n",
        "            match = re.search(r'\\b' + re.escape(keyword) + r'\\b', full_text[abstract_start.end():], re.IGNORECASE)\n",
        "            if match:\n",
        "                abstract_end = abstract_start.end() + match.start()\n",
        "                break\n",
        "\n",
        "        full_text_cleaned = full_text[:abstract_start.start()] + full_text[abstract_end:]\n",
        "\n",
        "    return full_text_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21edeb7f",
      "metadata": {
        "id": "21edeb7f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fetch_and_clean_full_text(paper_id, abstract, category_dir, retries=5, delay=10):\n",
        "    metadata = fetch_metadata(paper_id, retries, delay)\n",
        "    if metadata:\n",
        "        pdf_url = parse_pdf_url(metadata)\n",
        "        if pdf_url:\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    response = requests.get(pdf_url, timeout=20)\n",
        "                    if response.status_code == 200:\n",
        "                        pdf_path = os.path.join(category_dir, f\"pdfs/{paper_id}.pdf\")\n",
        "                        with open(pdf_path, 'wb') as f:\n",
        "                            f.write(response.content)\n",
        "\n",
        "                        try:\n",
        "                            full_text = extract_text(pdf_path)\n",
        "                            full_text_cleaned = clean_text(full_text)\n",
        "                            full_text_cleaned = remove_abstract_from_full_text(full_text_cleaned, abstract)\n",
        "\n",
        "                            if full_text_cleaned:\n",
        "                                txt_path = os.path.join(category_dir, f\"texts/{paper_id}.txt\")\n",
        "                                with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "                                    f.write(full_text_cleaned)\n",
        "                                return full_text_cleaned\n",
        "                        except PSSyntaxError as e:\n",
        "                            print(f\"PSSyntaxError for paper_id {paper_id}: {e}\")\n",
        "                            return None\n",
        "                    elif response.status_code == 500:\n",
        "                        print(f\"HTTP 500 error for paper_id {paper_id}, retrying...\")\n",
        "                        time.sleep(delay)\n",
        "                    else:\n",
        "                        print(f\"Failed to fetch PDF for paper_id {paper_id}, Status code: {response.status_code}\")\n",
        "                        return None\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Attempt {attempt + 1} to fetch PDF failed: {e}\")\n",
        "                    if attempt < retries - 1:\n",
        "                        time.sleep(delay)\n",
        "                    else:\n",
        "                        print(f\"Failed to fetch PDF for paper_id {paper_id} after {retries} attempts.\")\n",
        "                        return None\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e264759b",
      "metadata": {
        "id": "e264759b"
      },
      "outputs": [],
      "source": [
        "def fetch_full_texts_by_category(df, category, chunk_size=50):\n",
        "    filtered_df = df[df['primary_category_full'] == category].copy()\n",
        "    num_chunks = (len(filtered_df) + chunk_size - 1) // chunk_size\n",
        "\n",
        "    category_dir = os.path.join(\"full\", category)\n",
        "    os.makedirs(os.path.join(category_dir, \"pdfs\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(category_dir, \"texts\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(category_dir, \"chunks\"), exist_ok=True)\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        chunk_df = filtered_df.iloc[i * chunk_size:(i + 1) * chunk_size].copy()\n",
        "        full_texts = []\n",
        "\n",
        "        for paper_id, abstract in zip(chunk_df['paper_id'], chunk_df['abstract']):\n",
        "            full_text = fetch_and_clean_full_text(paper_id, abstract, category_dir)\n",
        "            full_texts.append(full_text)\n",
        "\n",
        "        chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
        "        chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n",
        "\n",
        "        output_csv = os.path.join(category_dir, f\"chunks/cleaned_texts_chunk_{i + 1+16}.csv\")\n",
        "        chunk_df.to_csv(output_csv, index=False, escapechar='\\\\')\n",
        "\n",
        "        print(f\"Saved chunk {i + 1} to {output_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f94f13",
      "metadata": {
        "id": "28f94f13"
      },
      "outputs": [],
      "source": [
        "categories = ['Artificial Intelligence', 'Computer Vision and Pattern Recognition']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "350f6d5b",
      "metadata": {
        "scrolled": true,
        "id": "350f6d5b",
        "outputId": "5a943688-8839-4931-9a19-b8d567ceae53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing category: Artificial Intelligence\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Failed to fetch PDF for paper_id 2212.13631v4, Status code: 500\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 1 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_1.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 2 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_2.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2212.08966v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282EA22B190>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 3 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_3.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2212.02064v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282EB1F27A0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Failed to fetch PDF for paper_id 2211.16242v2, Status code: 404\n",
            "Saved chunk 4 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_4.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 5 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_5.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 6 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_6.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n",
            "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n",
            "Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start and end byte have different lengths.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch PDF for paper_id 2211.03888v2, Status code: 500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 7 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_7.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2211.01496v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282814A1120>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 8 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_8.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 9 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_9.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 10 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_10.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 11 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_11.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 3 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 4 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 5 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Failed to fetch PDF for paper_id 2209.14292v3 after 5 attempts.\n",
            "Failed to fetch PDF for paper_id 2209.13002v2, Status code: 404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 12 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_12.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 13 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_13.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 14 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_14.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 15 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_15.csv\n",
            "Failed to fetch PDF for paper_id 2208.14037v3, Status code: 404\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 16 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_16.csv\n"
          ]
        },
        {
          "ename": "PSSyntaxError",
          "evalue": "Invalid dictionary construct: [/'Producer', b'pdfTeX-1.40.21', /'Title', b'KRACL: Contrastive Learning with Graph Context Modeling for Sparse Knowledge Graph Completion', /'Author', /b'Zhaouxan', /b'Tan,', /b'Zilong', /b'Chen,', /b'Shangbin', /b'Feng,', /b'Qingyue', /b'Zhang,', /b'Qinghua', /b'Zheng,', /b'Jundong', /b'Li,', /b'Minnan', /b'Luo', /'TemplateVersion', b'2023.1', /'Author', b'', /'Title', b'', /'Subject', b'', /'Creator', b'LaTeX with hyperref', /'Keywords', b'', /'CreationDate', b'D:20220817003034Z', /'ModDate', b'D:20220817003034Z', /'Trapped', /'False', /'PTEX.Fullbanner', b'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2']",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPSSyntaxError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17044\\2642867012.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Processing category: {category}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfetch_full_texts_by_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs_papers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Finished processing category: {category}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py\u001b[0m in \u001b[0;36mfetch_full_texts_by_category\u001b[1;34m(df, category, chunk_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpaper_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paper_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mfull_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_and_clean_full_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaper_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mfull_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17044\\389304768.py\u001b[0m in \u001b[0;36mfetch_and_clean_full_text\u001b[1;34m(paper_id, abstract, category_dir, retries, delay)\u001b[0m\n\u001b[0;32m     12\u001b[0m                             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                         \u001b[0mfull_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                         \u001b[0mfull_text_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                         \u001b[0mfull_text_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_abstract_from_full_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text_cleaned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\high_level.py\u001b[0m in \u001b[0;36mextract_text\u001b[1;34m(pdf_file, password, page_numbers, maxpages, caching, codec, laparams)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0minterpreter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPDFPageInterpreter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrsrcmgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         for page in PDFPage.get_pages(\n\u001b[0m\u001b[0;32m    169\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mpage_numbers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfpage.py\u001b[0m in \u001b[0;36mget_pages\u001b[1;34m(cls, fp, pagenos, maxpages, password, caching, check_extractable)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPDFParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;31m# Create a PDF document object that stores the document structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPDFDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;31m# Check if the document allows text extraction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;31m# If not, warn the user and proceed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfdocument.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parser, password, caching, fallback)\u001b[0m\n\u001b[0;32m    744\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_password\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpassword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"Info\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrailer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrailer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Info\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"Root\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrailer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 \u001b[1;31m# Every PDF file must have exactly one /Root dictionary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdftypes.py\u001b[0m in \u001b[0;36mdict_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdict_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolve1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTRICT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdftypes.py\u001b[0m in \u001b[0;36mresolve1\u001b[1;34m(x, default)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \"\"\"\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPDFObjRef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdftypes.py\u001b[0m in \u001b[0;36mresolve\u001b[1;34m(self, default)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPDFObjectNotFound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfdocument.py\u001b[0m in \u001b[0;36mgetobj\u001b[1;34m(self, objid)\u001b[0m\n\u001b[0;32m    864\u001b[0m                         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getobj_objstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getobj_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecipher\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m                             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecipher_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecipher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfdocument.py\u001b[0m in \u001b[0;36m_getobj_parse\u001b[1;34m(self, pos, objid)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwd\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mKWD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"obj\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mPDFSyntaxError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid object spec: offset=%r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnextobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\psparser.py\u001b[0m in \u001b[0;36mnextobject\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                         \u001b[0merror_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Invalid dictionary construct: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mPSSyntaxError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m                     d = {\n\u001b[0;32m    634\u001b[0m                         \u001b[0mliteral_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mPSSyntaxError\u001b[0m: Invalid dictionary construct: [/'Producer', b'pdfTeX-1.40.21', /'Title', b'KRACL: Contrastive Learning with Graph Context Modeling for Sparse Knowledge Graph Completion', /'Author', /b'Zhaouxan', /b'Tan,', /b'Zilong', /b'Chen,', /b'Shangbin', /b'Feng,', /b'Qingyue', /b'Zhang,', /b'Qinghua', /b'Zheng,', /b'Jundong', /b'Li,', /b'Minnan', /b'Luo', /'TemplateVersion', b'2023.1', /'Author', b'', /'Title', b'', /'Subject', b'', /'Creator', b'LaTeX with hyperref', /'Keywords', b'', /'CreationDate', b'D:20220817003034Z', /'ModDate', b'D:20220817003034Z', /'Trapped', /'False', /'PTEX.Fullbanner', b'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2']"
          ]
        }
      ],
      "source": [
        "for category in categories:\n",
        "    print(f\"Processing category: {category}\")\n",
        "    fetch_full_texts_by_category(cs_papers, category, chunk_size=50)\n",
        "    print(f\"Finished processing category: {category}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea857056",
      "metadata": {
        "id": "ea857056"
      },
      "outputs": [],
      "source": [
        "categories2 = ['General Literature', 'Operating Systems', 'Programming Languages']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8870697f",
      "metadata": {
        "id": "8870697f",
        "outputId": "20c74c45-0d02-4c53-f740-d45a26254158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing category: General Literature\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 1 to full\\General Literature\\chunks/cleaned_texts_chunk_1.csv\n",
            "Failed to fetch PDF for paper_id 1012.4170v2, Status code: 404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The PDF <_io.BufferedReader name='full\\\\General Literature\\\\pdfs/0808.3717v1.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch metadata for paper_id 0702141v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610127v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608062v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0607022v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0602070v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0412090v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411009v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0410075v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0404033v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0404026v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0402037v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0306132v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0210001v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0110018v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0106022v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0101011v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0012003v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9911005v1, Status code: 400\n",
            "Saved chunk 2 to full\\General Literature\\chunks/cleaned_texts_chunk_2.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch metadata for paper_id 9811005v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809010v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9301114v1, Status code: 400\n",
            "Saved chunk 3 to full\\General Literature\\chunks/cleaned_texts_chunk_3.csv\n",
            "Finished processing category: General Literature\n",
            "Processing category: Operating Systems\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch metadata for paper_id 0701021v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9903014v1, Status code: 400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 1 to full\\Operating Systems\\chunks/cleaned_texts_chunk_1.csv\n",
            "Failed to fetch PDF for paper_id 2104.05306v3, Status code: 404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 2 to full\\Operating Systems\\chunks/cleaned_texts_chunk_2.csv\n",
            "Failed to fetch PDF for paper_id 1909.11644v2, Status code: 404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 3 to full\\Operating Systems\\chunks/cleaned_texts_chunk_3.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 4 to full\\Operating Systems\\chunks/cleaned_texts_chunk_4.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1705.06932v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282816BECE0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_full_text'] = full_texts\n",
            "C:\\Users\\amins\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  chunk_df.loc[:, 'cleaned_abstract'] = chunk_df['abstract'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 5 to full\\Operating Systems\\chunks/cleaned_texts_chunk_5.csv\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "('Unhandled', 14)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17044\\1645853137.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategories2\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Processing category: {category}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfetch_full_texts_by_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs_papers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Finished processing category: {category}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17044\\559665559.py\u001b[0m in \u001b[0;36mfetch_full_texts_by_category\u001b[1;34m(df, category, chunk_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpaper_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paper_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mfull_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_and_clean_full_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaper_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mfull_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17044\\389304768.py\u001b[0m in \u001b[0;36mfetch_and_clean_full_text\u001b[1;34m(paper_id, abstract, category_dir, retries, delay)\u001b[0m\n\u001b[0;32m     12\u001b[0m                             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                         \u001b[0mfull_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                         \u001b[0mfull_text_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                         \u001b[0mfull_text_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_abstract_from_full_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text_cleaned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\high_level.py\u001b[0m in \u001b[0;36mextract_text\u001b[1;34m(pdf_file, password, page_numbers, maxpages, caching, codec, laparams)\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mcaching\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         ):\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0minterpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mprocess_page\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m    995\u001b[0m             \u001b[0mctm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0my0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_contents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    998\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mrender_contents\u001b[1;34m(self, resources, streams, ctm)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[1;34m\"render_contents: resources=%r, streams=%r, ctm=%r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         )\n\u001b[1;32m-> 1014\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_resources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstreams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36minit_resources\u001b[1;34m(self, resources)\u001b[0m\n\u001b[0;32m    382\u001b[0m                         \u001b[0mobjid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                     \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfontmap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfontid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsrcmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_font\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"ColorSpace\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcsid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdict_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfinterp.py\u001b[0m in \u001b[0;36mget_font\u001b[1;34m(self, objid, spec)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0msubtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"CIDFontType0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"CIDFontType2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# CID Font\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                 \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPDFCIDFont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0msubtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Type0\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[1;31m# Type0 Font\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdffont.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rsrcmgr, spec, strict)\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mttf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0municode_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mttf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_unicode_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1098\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTrueTypeFont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCMapNotFound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdffont.py\u001b[0m in \u001b[0;36mcreate_unicode_map\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    828\u001b[0m                             \u001b[0mchar2gid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0midd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFFFF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 830\u001b[1;33m                 \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unhandled\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmttype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    831\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchar2gid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTrueTypeFont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCMapNotFound\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAssertionError\u001b[0m: ('Unhandled', 14)"
          ]
        }
      ],
      "source": [
        "for category in categories2 :\n",
        "    print(f\"Processing category: {category}\")\n",
        "    fetch_full_texts_by_category(cs_papers, category, chunk_size=50)\n",
        "    print(f\"Finished processing category: {category}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dad59ef",
      "metadata": {
        "id": "1dad59ef"
      },
      "outputs": [],
      "source": [
        "categories3 = ['Computer Vision and Pattern Recognition', 'Data Structures and Algorithms']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "770d0ed4",
      "metadata": {
        "id": "770d0ed4"
      },
      "outputs": [],
      "source": [
        "for category in categories3 :\n",
        "    print(f\"Processing category: {category}\")\n",
        "    fetch_full_texts_by_category(cs_papers, category, chunk_size=50)\n",
        "    print(f\"Finished processing category: {category}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d863b0",
      "metadata": {
        "id": "f0d863b0",
        "outputId": "42eadafd-e092-4329-abca-83370d3e515d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 1 to data2\\Machine Learning\\chunks/cleaned_texts_chunk_1.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2301.00717v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x0000028282540B20>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2212.12015v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282827ECA90>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 2 to data2\\Machine Learning\\chunks/cleaned_texts_chunk_2.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2212.09920v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282880EE4A0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2212.09840v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x0000028282254850>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Attempt 2 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 3 to data2\\Machine Learning\\chunks/cleaned_texts_chunk_3.csv\n"
          ]
        },
        {
          "ename": "PSSyntaxError",
          "evalue": "Invalid dictionary construct: [/'Producer', b'pdfTeX-1.40.21', /'Title', /b'Scaling', /b'Marginalized', /b'Importance', /b'Sampling', /b'to', /b'High-Dimensional', /b'State-Spaces', /b'via', /b'State', /b'Abstraction', /'TemplateVersion', b'2023.1', /'Creator', b'TeX', /'CreationDate', b'D:20221216012230Z', /'ModDate', b'D:20221216012230Z', /'Trapped', /'False', /'PTEX.Fullbanner', b'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2']",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPSSyntaxError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17044\\3367486486.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfetch_full_texts_by_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs_papers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Machine Learning'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17044\\405355140.py\u001b[0m in \u001b[0;36mfetch_full_texts_by_category\u001b[1;34m(df, category, chunk_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpaper_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paper_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mfull_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_and_clean_full_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaper_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mfull_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17044\\389304768.py\u001b[0m in \u001b[0;36mfetch_and_clean_full_text\u001b[1;34m(paper_id, abstract, category_dir, retries, delay)\u001b[0m\n\u001b[0;32m     12\u001b[0m                             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                         \u001b[0mfull_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                         \u001b[0mfull_text_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                         \u001b[0mfull_text_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_abstract_from_full_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text_cleaned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabstract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\high_level.py\u001b[0m in \u001b[0;36mextract_text\u001b[1;34m(pdf_file, password, page_numbers, maxpages, caching, codec, laparams)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0minterpreter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPDFPageInterpreter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrsrcmgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         for page in PDFPage.get_pages(\n\u001b[0m\u001b[0;32m    169\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mpage_numbers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfpage.py\u001b[0m in \u001b[0;36mget_pages\u001b[1;34m(cls, fp, pagenos, maxpages, password, caching, check_extractable)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPDFParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;31m# Create a PDF document object that stores the document structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPDFDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;31m# Check if the document allows text extraction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;31m# If not, warn the user and proceed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfdocument.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, parser, password, caching, fallback)\u001b[0m\n\u001b[0;32m    744\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_password\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpassword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"Info\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrailer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrailer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Info\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"Root\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrailer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 \u001b[1;31m# Every PDF file must have exactly one /Root dictionary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdftypes.py\u001b[0m in \u001b[0;36mdict_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdict_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolve1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTRICT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdftypes.py\u001b[0m in \u001b[0;36mresolve1\u001b[1;34m(x, default)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \"\"\"\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPDFObjRef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdftypes.py\u001b[0m in \u001b[0;36mresolve\u001b[1;34m(self, default)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPDFObjectNotFound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfdocument.py\u001b[0m in \u001b[0;36mgetobj\u001b[1;34m(self, objid)\u001b[0m\n\u001b[0;32m    864\u001b[0m                         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getobj_objstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getobj_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecipher\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m                             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecipher_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecipher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\pdfdocument.py\u001b[0m in \u001b[0;36m_getobj_parse\u001b[1;34m(self, pos, objid)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwd\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mKWD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"obj\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mPDFSyntaxError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid object spec: offset=%r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnextobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\shan\\lib\\site-packages\\pdfminer\\psparser.py\u001b[0m in \u001b[0;36mnextobject\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                         \u001b[0merror_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Invalid dictionary construct: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mPSSyntaxError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m                     d = {\n\u001b[0;32m    634\u001b[0m                         \u001b[0mliteral_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mPSSyntaxError\u001b[0m: Invalid dictionary construct: [/'Producer', b'pdfTeX-1.40.21', /'Title', /b'Scaling', /b'Marginalized', /b'Importance', /b'Sampling', /b'to', /b'High-Dimensional', /b'State-Spaces', /b'via', /b'State', /b'Abstraction', /'TemplateVersion', b'2023.1', /'Creator', b'TeX', /'CreationDate', b'D:20221216012230Z', /'ModDate', b'D:20221216012230Z', /'Trapped', /'False', /'PTEX.Fullbanner', b'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2']"
          ]
        }
      ],
      "source": [
        "fetch_full_texts_by_category(cs_papers, 'Machine Learning', chunk_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a14d4fa5",
      "metadata": {
        "id": "a14d4fa5"
      },
      "outputs": [],
      "source": [
        "categories = ['Artificial Intelligence', 'Computer Vision and Pattern Recognition']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5745d268",
      "metadata": {
        "id": "5745d268"
      },
      "outputs": [],
      "source": [
        "cs_ai = cs_papers[cs_papers.primary_category_full == 'Artificial Intelligence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ecb99c",
      "metadata": {
        "id": "31ecb99c",
        "outputId": "3fde3c14-1917-4665-f042-774160c05792"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>year</th>\n",
              "      <th>primary_category</th>\n",
              "      <th>categories</th>\n",
              "      <th>primary_category_full</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5541</th>\n",
              "      <td>2208.09568v1</td>\n",
              "      <td>Probabilities of Causation with Nonbinary Trea...</td>\n",
              "      <td>This paper deals with the problem of estimatin...</td>\n",
              "      <td>2022-08-19 23:54:47+00:00</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5543</th>\n",
              "      <td>2208.09558v1</td>\n",
              "      <td>Personalized Decision Making -- A Conceptual I...</td>\n",
              "      <td>Personalized decision making targets the behav...</td>\n",
              "      <td>2022-08-19 22:21:29+00:00</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>cs.AI cs.LO</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5544</th>\n",
              "      <td>2208.09554v1</td>\n",
              "      <td>Evaluating Diverse Knowledge Sources for Onlin...</td>\n",
              "      <td>Online autonomous agents are able to draw on a...</td>\n",
              "      <td>2022-08-19 21:53:15+00:00</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5554</th>\n",
              "      <td>2208.09344v1</td>\n",
              "      <td>Positive dependence in qualitative probabilist...</td>\n",
              "      <td>Qualitative probabilistic networks (QPNs) comb...</td>\n",
              "      <td>2022-08-19 13:53:04+00:00</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5560</th>\n",
              "      <td>2208.09292v3</td>\n",
              "      <td>UnCommonSense: Informative Negative Knowledge ...</td>\n",
              "      <td>Commonsense knowledge about everyday concepts ...</td>\n",
              "      <td>2022-09-05 07:02:33+00:00</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>cs.AI cs.IR</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198423</th>\n",
              "      <td>1003.0659v2</td>\n",
              "      <td>Particle Filtering on the Audio Localization M...</td>\n",
              "      <td>We present a novel particle filtering algorith...</td>\n",
              "      <td>2010-03-02 21:40:35+00:00</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>cs.AI cs.SD</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198438</th>\n",
              "      <td>1301.2306v1</td>\n",
              "      <td>A Mixed Graphical Model for Rhythmic Parsing</td>\n",
              "      <td>A method is presented for the rhythmic parsing...</td>\n",
              "      <td>2013-01-10 16:26:12+00:00</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>cs.AI cs.SD</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198454</th>\n",
              "      <td>1404.2313v1</td>\n",
              "      <td>Outer-Product Hidden Markov Model and Polyphon...</td>\n",
              "      <td>We present a polyphonic MIDI score-following a...</td>\n",
              "      <td>2014-04-08 21:48:13+00:00</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>cs.AI cs.SD</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198455</th>\n",
              "      <td>1404.2314v2</td>\n",
              "      <td>A Stochastic Temporal Model of Polyphonic MIDI...</td>\n",
              "      <td>We study indeterminacies in realization of orn...</td>\n",
              "      <td>2016-08-03 00:44:16+00:00</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>cs.AI cs.SD</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198464</th>\n",
              "      <td>1412.3079v1</td>\n",
              "      <td>Computoser - rule-based, probability-driven al...</td>\n",
              "      <td>This paper presents the Computoser hybrid prob...</td>\n",
              "      <td>2014-12-09 20:06:10+00:00</td>\n",
              "      <td>cs.AI</td>\n",
              "      <td>cs.AI cs.SD</td>\n",
              "      <td>Artificial Intelligence</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5032 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            paper_id                                              title  \\\n",
              "5541    2208.09568v1  Probabilities of Causation with Nonbinary Trea...   \n",
              "5543    2208.09558v1  Personalized Decision Making -- A Conceptual I...   \n",
              "5544    2208.09554v1  Evaluating Diverse Knowledge Sources for Onlin...   \n",
              "5554    2208.09344v1  Positive dependence in qualitative probabilist...   \n",
              "5560    2208.09292v3  UnCommonSense: Informative Negative Knowledge ...   \n",
              "...              ...                                                ...   \n",
              "198423   1003.0659v2  Particle Filtering on the Audio Localization M...   \n",
              "198438   1301.2306v1       A Mixed Graphical Model for Rhythmic Parsing   \n",
              "198454   1404.2313v1  Outer-Product Hidden Markov Model and Polyphon...   \n",
              "198455   1404.2314v2  A Stochastic Temporal Model of Polyphonic MIDI...   \n",
              "198464   1412.3079v1  Computoser - rule-based, probability-driven al...   \n",
              "\n",
              "                                                 abstract  \\\n",
              "5541    This paper deals with the problem of estimatin...   \n",
              "5543    Personalized decision making targets the behav...   \n",
              "5544    Online autonomous agents are able to draw on a...   \n",
              "5554    Qualitative probabilistic networks (QPNs) comb...   \n",
              "5560    Commonsense knowledge about everyday concepts ...   \n",
              "...                                                   ...   \n",
              "198423  We present a novel particle filtering algorith...   \n",
              "198438  A method is presented for the rhythmic parsing...   \n",
              "198454  We present a polyphonic MIDI score-following a...   \n",
              "198455  We study indeterminacies in realization of orn...   \n",
              "198464  This paper presents the Computoser hybrid prob...   \n",
              "\n",
              "                             year primary_category   categories  \\\n",
              "5541    2022-08-19 23:54:47+00:00            cs.AI        cs.AI   \n",
              "5543    2022-08-19 22:21:29+00:00            cs.AI  cs.AI cs.LO   \n",
              "5544    2022-08-19 21:53:15+00:00            cs.AI        cs.AI   \n",
              "5554    2022-08-19 13:53:04+00:00            cs.AI        cs.AI   \n",
              "5560    2022-09-05 07:02:33+00:00            cs.AI  cs.AI cs.IR   \n",
              "...                           ...              ...          ...   \n",
              "198423  2010-03-02 21:40:35+00:00            cs.AI  cs.AI cs.SD   \n",
              "198438  2013-01-10 16:26:12+00:00            cs.AI  cs.AI cs.SD   \n",
              "198454  2014-04-08 21:48:13+00:00            cs.AI  cs.AI cs.SD   \n",
              "198455  2016-08-03 00:44:16+00:00            cs.AI  cs.AI cs.SD   \n",
              "198464  2014-12-09 20:06:10+00:00            cs.AI  cs.AI cs.SD   \n",
              "\n",
              "          primary_category_full  \n",
              "5541    Artificial Intelligence  \n",
              "5543    Artificial Intelligence  \n",
              "5544    Artificial Intelligence  \n",
              "5554    Artificial Intelligence  \n",
              "5560    Artificial Intelligence  \n",
              "...                         ...  \n",
              "198423  Artificial Intelligence  \n",
              "198438  Artificial Intelligence  \n",
              "198454  Artificial Intelligence  \n",
              "198455  Artificial Intelligence  \n",
              "198464  Artificial Intelligence  \n",
              "\n",
              "[5032 rows x 7 columns]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cs_ai_shape = cs_ai.shape\n",
        "ai_index = cs_ai_shape[0]- 800\n",
        "cs_ai_tail = cs_ai.tail(ai_index)\n",
        "cs_ai_tail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d444203",
      "metadata": {
        "id": "2d444203",
        "outputId": "4208cfc6-20e5-4c55-ba67-44ab9de1b0dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing category: Artificial Intelligence\n",
            "Saved chunk 1 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_17.csv\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
            "\n",
            "Saved chunk 2 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_18.csv\n",
            "Saved chunk 3 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_19.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 4 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_20.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2207.05188v3 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21AB00>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 5 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_21.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2207.01058v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21B580>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 6 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_22.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 7 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_23.csv\n",
            "Failed to fetch PDF for paper_id 2206.05731v2, Status code: 404\n",
            "Saved chunk 8 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_24.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 9 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_25.csv\n",
            "MuPDF error: syntax error: could not parse color space (94 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (272 0 R)\n",
            "\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 10 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_26.csv\n",
            "MuPDF error: syntax error: could not parse color space (337 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (337 0 R)\n",
            "\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2205.15292v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B33D0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 11 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_27.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2205.08438v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F16B4C0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 12 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_28.csv\n",
            "MuPDF error: syntax error: could not parse color space (3629 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (3792 0 R)\n",
            "\n",
            "Saved chunk 13 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_29.csv\n",
            "Saved chunk 14 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_30.csv\n",
            "MuPDF error: syntax error: could not parse color space (2692 0 R)\n",
            "\n",
            "Failed to fetch PDF for paper_id 2005.00153v2, Status code: 404\n",
            "HTTP 500 error for paper_id 1910.13641v1, retrying...\n",
            "HTTP 500 error for paper_id 1910.13641v1, retrying...\n",
            "HTTP 500 error for paper_id 1910.13641v1, retrying...\n",
            "Attempt 4 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 5 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Failed to fetch PDF for paper_id 1910.13641v1 after 5 attempts.\n",
            "Saved chunk 15 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_31.csv\n",
            "Saved chunk 16 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_32.csv\n",
            "Saved chunk 17 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_33.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "HTTP 500 error for paper_id 2112.14770v1, retrying...\n",
            "HTTP 500 error for paper_id 2112.14770v1, retrying...\n",
            "HTTP 500 error for paper_id 2112.14770v1, retrying...\n",
            "Failed to fetch metadata for paper_id 0406025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0403009v2, Status code: 400\n",
            "Saved chunk 18 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_34.csv\n",
            "Saved chunk 19 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_35.csv\n",
            "Saved chunk 20 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_36.csv\n",
            "Saved chunk 21 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_37.csv\n",
            "Saved chunk 22 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_38.csv\n",
            "MuPDF error: syntax error: could not parse color space (245 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (341 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (567 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (2851 0 R)\n",
            "\n",
            "Saved chunk 23 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_39.csv\n",
            "Failed to fetch PDF for paper_id 1608.05513v2, Status code: 404\n",
            "Saved chunk 24 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_40.csv\n",
            "Saved chunk 25 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_41.csv\n",
            "Saved chunk 26 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_42.csv\n",
            "MuPDF error: syntax error: could not parse color space (498 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (574 0 R)\n",
            "\n",
            "Failed to fetch PDF for paper_id 2210.08731v3, Status code: 404\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 27 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_43.csv\n",
            "Saved chunk 28 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_44.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2102.07548v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21B7C0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 29 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_45.csv\n",
            "Failed to fetch PDF for paper_id 2008.08300v2, Status code: 404\n",
            "Saved chunk 30 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_46.csv\n",
            "Failed to fetch PDF for paper_id 1906.09689v2, Status code: 404\n",
            "Saved chunk 31 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_47.csv\n",
            "Saved chunk 32 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_48.csv\n",
            "Failed to fetch PDF for paper_id 1812.04562v4, Status code: 404\n",
            "Saved chunk 33 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_49.csv\n",
            "Failed to fetch metadata for paper_id 0611009v1, Status code: 400\n",
            "Saved chunk 34 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_50.csv\n",
            "Failed to fetch metadata for paper_id 0607016v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0601051v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411016v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408023v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0211035v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0211004v3, Status code: 400\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch metadata for paper_id 0207008v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0106004v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003076v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0002016v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9909010v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9909009v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9810018v1, Status code: 400\n",
            "Saved chunk 35 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_51.csv\n",
            "Saved chunk 36 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_52.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Saved chunk 37 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_53.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Saved chunk 38 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_54.csv\n",
            "Failed to fetch metadata for paper_id 0609120v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0606024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0511013v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0406055v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405007v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0305033v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0305022v1, Status code: 400\n",
            "Saved chunk 39 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_55.csv\n",
            "Failed to fetch metadata for paper_id 0703091v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0412079v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0412077v1, Status code: 400\n",
            "Saved chunk 40 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_56.csv\n",
            "MuPDF error: syntax error: could not parse color space (117 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (235 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (407 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (10972 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (273 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (325 0 R)\n",
            "\n",
            "Saved chunk 41 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_57.csv\n",
            "HTTP 500 error for paper_id 2012.02108v2, retrying...\n",
            "HTTP 500 error for paper_id 2012.02108v2, retrying...\n",
            "HTTP 500 error for paper_id 2012.02108v2, retrying...\n",
            "HTTP 500 error for paper_id 2012.02108v2, retrying...\n",
            "HTTP 500 error for paper_id 2012.02108v2, retrying...\n",
            "HTTP 500 error for paper_id 2012.01022v2, retrying...\n",
            "HTTP 500 error for paper_id 2012.01022v2, retrying...\n",
            "Attempt 3 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 4 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "HTTP 500 error for paper_id 2012.01022v2, retrying...\n",
            "Saved chunk 42 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_58.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "HTTP 500 error for paper_id 2001.05375v1, retrying...\n",
            "HTTP 500 error for paper_id 2001.05375v1, retrying...\n",
            "HTTP 500 error for paper_id 2001.05375v1, retrying...\n",
            "Saved chunk 43 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_59.csv\n",
            "Failed to fetch PDF for paper_id 1805.08347v3, Status code: 404\n",
            "Saved chunk 44 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_60.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 45 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_61.csv\n",
            "Saved chunk 46 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_62.csv\n",
            "Saved chunk 47 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_63.csv\n",
            "Saved chunk 48 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_64.csv\n",
            "Failed to fetch PDF for paper_id 0912.5073v2, Status code: 404\n",
            "Failed to fetch metadata for paper_id 0611163v1, Status code: 400\n",
            "Saved chunk 49 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_65.csv\n",
            "Failed to fetch metadata for paper_id 0606027v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605138v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605120v1, Status code: 400\n",
            "Saved chunk 50 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_66.csv\n",
            "Failed to fetch metadata for paper_id 0204053v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0202004v3, Status code: 400\n",
            "Failed to fetch PDF for paper_id 2302.00496v3, Status code: 404\n",
            "Saved chunk 51 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_67.csv\n",
            "Saved chunk 52 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_68.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 53 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_69.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Saved chunk 54 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_70.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Saved chunk 55 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_71.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2003.07520v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B3DF0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 56 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_72.csv\n",
            "Saved chunk 57 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_73.csv\n",
            "HTTP 500 error for paper_id 1812.03508v3, retrying...\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 3 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 4 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 5 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Failed to fetch PDF for paper_id 1812.03508v3 after 5 attempts.\n",
            "Failed to fetch PDF for paper_id 1810.06985v8, Status code: 404\n",
            "Saved chunk 58 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_74.csv\n",
            "Failed to fetch PDF for paper_id 1710.05341v2, Status code: 404\n",
            "Saved chunk 59 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_75.csv\n",
            "Saved chunk 60 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_76.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Saved chunk 61 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_77.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 3 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Saved chunk 62 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_78.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 63 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_79.csv\n",
            "MuPDF error: syntax error: could not parse color space (208 0 R)\n",
            "\n",
            "Saved chunk 64 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_80.csv\n",
            "Saved chunk 65 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_81.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Saved chunk 66 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_82.csv\n",
            "Failed to fetch PDF for paper_id 2008.06464v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 2003.07108v2, Status code: 404\n",
            "Saved chunk 67 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_83.csv\n",
            "Failed to fetch PDF for paper_id 2003.03917v3, Status code: 404\n",
            "Saved chunk 68 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_84.csv\n",
            "Saved chunk 69 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_85.csv\n",
            "Saved chunk 70 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_86.csv\n",
            "Saved chunk 71 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_87.csv\n",
            "Failed to fetch metadata for paper_id 0610167v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605031v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0508032v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504108v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0310010v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0211028v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0210030v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0205016v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0203021v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0106054v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0105025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003014v2, Status code: 400\n",
            "Saved chunk 72 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_88.csv\n",
            "MuPDF error: format error: invalid code in 2d faxd\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '9411.5827.93'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'h7.457'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '28.297252.677'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '785.797100.96'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '214..59'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '18.001785.076'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'h74.859'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '2.67.507'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '6252.94163110.73'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '2.67.50741631.43044.304'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '693.5430443.46'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '4124.348.539'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '4l'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '44.24149.687.73'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '2.674149.687.73'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '47.4149.68'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '10079c46'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '808.9.811'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '2.677m'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'c7'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '549.98l'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '5493.97223.25c'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '825.609253.195'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '39.6879451.617'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'c3547.1'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '4191.19238.773'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '54.0l309'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '28.297250243.47617297257.5239'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '9411.5h'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'c416'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '78.25484.'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'l9.988549.1'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '429.98l'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '621.1l48'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '429.98l42.578'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'l32.49213.70148'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '596.9.21'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '596.90290.062938.25480l'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '246.10290.5.512'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '41.2810290.5.51420.109'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '.l2.7426'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '804.7.6875'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '4194297.7.6875'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '100.c75'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '865.l2.742670.5'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '78.25c54'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '6..507'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '6..507'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '78.25480619.9'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '7853.9290619.9'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '865.c9290619.'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '421862938.5.54'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '225.92938.5.5007'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '38.5.5'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '903.92942.229328'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '99.932942.22909'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '865.l2.942.22907853.92942.457'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '78.254827.687'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '78.25c54827.687'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '6..507'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '6..507'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'ch'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '805.770.9188801.29451..918884.649451..967'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '646.9451..9187796.38.770.9187796.38.71'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '94.6187796.38213.70'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '64798.439918884.6'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '798.4399c9188801.298.4399189.9'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '429.27.2'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '796.38.67.5087'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '636.908884.6'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '636.9c9188801.28'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '636.9089.27.2'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '49.27.2'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'c45'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'l4c'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '628l'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '621.1l48'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '628l42.578'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'l58293'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '596.90539.6477'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '596.9.2137'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '596.905658.5393'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '630.7.2137'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '596.90314.8837'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '9.14938.254l'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '101.3l82'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '4306477.7.687l882'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '48.867.7.68752'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '100.c752'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '865.l2.2'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '4805407853.92531.8.'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '78.25c543720.664'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '6..507'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'c3409.94.9655301'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '6..507'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'c3537054'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'l3'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'l3'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '865.c925'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '613.19238.656'\n",
            "\n",
            "Failed to fetch PDF for paper_id 1304.6551v4, Status code: 404\n",
            "Saved chunk 73 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_89.csv\n",
            "Failed to fetch metadata for paper_id 0610095v1, Status code: 400\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch metadata for paper_id 0607037v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0606070v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0509032v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504101v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0503046v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0503043v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0502078v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0407034v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307017v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307013v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0306091v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0211031v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0210007v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207097v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207072v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0111038v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0012011v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0008008v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0005024v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0004005v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0512010v1, Status code: 400\n",
            "Saved chunk 74 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_90.csv\n",
            "Saved chunk 75 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_91.csv\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "Saved chunk 76 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_92.csv\n",
            "Failed to fetch metadata for paper_id 0701174v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0509025v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405107v1, Status code: 400\n",
            "Saved chunk 77 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_93.csv\n",
            "Saved chunk 78 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_94.csv\n",
            "Failed to fetch PDF for paper_id 1705.00673v2, Status code: 404\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Saved chunk 79 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_95.csv\n",
            "MuPDF error: syntax error: could not parse color space (275 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (392 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (694 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1005 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1440 0 R)\n",
            "\n",
            "Failed to fetch metadata for paper_id 0405004v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0206027v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0007002v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0112008v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0505080v1, Status code: 400\n",
            "Saved chunk 80 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_96.csv\n",
            "Failed to fetch metadata for paper_id 9308101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9308102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9309101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9311101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9311102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9312101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9401101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9402101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9402102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9402103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9403101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9406101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9406102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9408101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9408102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9408103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9409101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9412101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9412102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9412103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9501101v1, Status code: 400\n",
            "Saved chunk 81 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_97.csv\n",
            "Failed to fetch metadata for paper_id 9501102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9501103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9503102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9504101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9505101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9505102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9505103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9505104v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9505105v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9506101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9506102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9507101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9508101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9508102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9510101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9510102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9510103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9511101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9512101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9512102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9512103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9512104v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9512105v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9512106v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9512107v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9601101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9602101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9602102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9603101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9603102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9603103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9603104v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9604101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9604102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9604103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9605101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9605102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9605103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9605104v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9605105v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9605106v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9606101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9606102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9608103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9608104v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9609101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9609102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9610101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9610102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9611101v1, Status code: 400\n",
            "Saved chunk 82 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_98.csv\n",
            "Failed to fetch metadata for paper_id 9612101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9612102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9612103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9701101v1, Status code: 400\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch metadata for paper_id 9701102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9703101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9704101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9705101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9705102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9706101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9706102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9707101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9707102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9707103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9709101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9709102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9711102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9711103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9711104v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9712101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9712102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9801101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9801102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9803101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9803102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9803103v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9805101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9806101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9806102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9808101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9810016v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9811024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9812010v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9812017v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9903016v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9906002v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9909003v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9910016v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9911012v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0002002v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0002003v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0002009v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003008v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003012v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003016v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003020v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003021v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003023v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003027v1, Status code: 400\n",
            "Saved chunk 83 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_99.csv\n",
            "Failed to fetch metadata for paper_id 0003028v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003029v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003030v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003031v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003032v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003033v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003034v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003037v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003038v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003039v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003042v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003044v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003047v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003049v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003051v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003052v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003059v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003061v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003077v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0003080v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0005031v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0006043v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0007004v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0010037v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0011012v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0011030v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0105022v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0106006v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0106007v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0106025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0107002v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0109006v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0111060v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0112015v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0201022v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0202021v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0202022v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0202024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0202025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0202026v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0202031v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0202033v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0203002v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0203003v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0203004v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0203005v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0203007v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0204032v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0205014v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0206003v1, Status code: 400\n",
            "Saved chunk 84 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_100.csv\n",
            "Failed to fetch metadata for paper_id 0206041v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207021v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207023v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207029v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207030v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207042v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207045v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207056v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207059v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207060v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207065v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207067v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207075v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207083v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0208017v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0208019v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0208034v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0209019v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0209022v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0210004v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0211008v4, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0211027v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0211038v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0211039v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0211040v1, Status code: 400\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch metadata for paper_id 0212025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0301006v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0301010v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0301023v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0302029v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0302036v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0302039v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0303006v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0303009v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0303018v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0305001v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0305019v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0305044v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0306124v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0306135v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307010v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307048v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307050v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307056v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307063v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0308002v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0309025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0310023v1, Status code: 400\n",
            "Saved chunk 85 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_101.csv\n",
            "Failed to fetch metadata for paper_id 0310044v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0310045v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0310047v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0310061v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0310062v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311004v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311007v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311026v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311027v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311045v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311051v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0312020v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0312040v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0312045v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0312053v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0401009v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0402033v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0402035v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0402057v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0403002v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0403006v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0404011v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0404012v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0404051v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405008v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405009v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405010v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405011v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405012v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405013v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405014v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405016v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405017v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405018v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405019v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405026v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405028v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405030v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405031v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405032v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405033v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405049v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405050v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405051v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405052v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405071v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405090v1, Status code: 400\n",
            "Saved chunk 86 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_102.csv\n",
            "Failed to fetch metadata for paper_id 0405106v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405113v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0406038v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0407008v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0407037v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0407040v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0407042v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0407044v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408010v5, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408021v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408044v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408055v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408064v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0409007v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0409040v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0410014v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0410033v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0410049v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0410050v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411015v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411034v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411071v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411072v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0412091v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0501068v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0501072v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0501084v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0501086v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0501089v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0501093v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0501094v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0501095v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0501096v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0502060v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504064v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504065v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504066v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504071v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0505018v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0505081v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0506031v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0507010v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0507023v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0507029v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0508132v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0509011v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0509033v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0510050v1, Status code: 400\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch metadata for paper_id 0510062v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0510063v1, Status code: 400\n",
            "Saved chunk 87 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_103.csv\n",
            "Failed to fetch metadata for paper_id 0510079v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0510083v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0510091v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0511004v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0511015v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0511091v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0512045v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0512047v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0512099v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0601001v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0601031v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0601052v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0601109v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0602022v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0602031v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0603025v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0603038v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0603081v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0603120v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0604009v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0604042v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0604070v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0604086v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605012v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605017v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605055v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605108v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605123v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0606020v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0606029v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0606066v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0606081v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0607005v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0607084v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0607086v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0607143v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0607147v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608002v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608019v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608028v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609111v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609132v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609136v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609142v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610006v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610015v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610023v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610043v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610060v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610111v3, Status code: 400\n",
            "Saved chunk 88 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_104.csv\n",
            "Failed to fetch metadata for paper_id 0610140v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610156v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610165v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610175v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611047v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611085v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611118v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611135v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611138v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611141v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612056v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612057v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612068v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612109v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0701013v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0701184v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0702028v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0702170v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0703060v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0703124v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0703130v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0703156v1, Status code: 400\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "Saved chunk 89 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_105.csv\n",
            "Failed to fetch PDF for paper_id 0803.1207v3, Status code: 404\n",
            "Failed to fetch PDF for paper_id 0806.4511v5, Status code: 404\n",
            "Saved chunk 90 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_106.csv\n",
            "Failed to fetch PDF for paper_id 0812.0885v4, Status code: 404\n",
            "Saved chunk 91 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_107.csv\n",
            "Failed to fetch PDF for paper_id 0904.2827v5, Status code: 404\n",
            "Saved chunk 92 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_108.csv\n",
            "HTTP 500 error for paper_id 0910.1404v1, retrying...\n",
            "HTTP 500 error for paper_id 0910.1404v1, retrying...\n",
            "HTTP 500 error for paper_id 0910.1404v1, retrying...\n",
            "HTTP 500 error for paper_id 0910.1404v1, retrying...\n",
            "HTTP 500 error for paper_id 0910.1404v1, retrying...\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=0912.4584v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21BF10>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 93 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_109.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1002.3023v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B2200>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Failed to fetch PDF for paper_id 1006.5511v2, Status code: 404\n",
            "Saved chunk 94 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_110.csv\n",
            "Failed to fetch PDF for paper_id 1008.5188v2, Status code: 404\n",
            "Saved chunk 95 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_111.csv\n",
            "Failed to fetch PDF for paper_id 1011.0330v3, Status code: 404\n",
            "Saved chunk 96 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_112.csv\n",
            "Failed to fetch PDF for paper_id 1103.5034v2, Status code: 404\n",
            "Saved chunk 97 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_113.csv\n",
            "Saved chunk 98 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_114.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 99 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_115.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Attempt 3 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "HTTP 500 error for paper_id 1107.1686v1, retrying...\n",
            "HTTP 500 error for paper_id 1107.1686v1, retrying...\n",
            "Saved chunk 100 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_116.csv\n",
            "Failed to fetch PDF for paper_id 1108.3711v2, Status code: 404\n",
            "Saved chunk 101 to full\\Artificial Intelligence\\chunks/cleaned_texts_chunk_117.csv\n",
            "Finished processing category: Artificial Intelligence\n",
            "Processing category: Computer Vision and Pattern Recognition\n",
            "Finished processing category: Computer Vision and Pattern Recognition\n"
          ]
        }
      ],
      "source": [
        "for category in categories:\n",
        "    print(f\"Processing category: {category}\")\n",
        "    fetch_full_texts_by_category(cs_ai_tail, category, chunk_size=50)\n",
        "    print(f\"Finished processing category: {category}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5818c8e3",
      "metadata": {
        "id": "5818c8e3"
      },
      "outputs": [],
      "source": [
        "def fetch_and_clean_full_text(paper_id, abstract, category_dir, retries=5, delay=10):\n",
        "    metadata = fetch_metadata(paper_id, retries, delay)\n",
        "    if metadata:\n",
        "        pdf_url = parse_pdf_url(metadata)\n",
        "        if pdf_url:\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    response = requests.get(pdf_url, timeout=20)\n",
        "                    if response.status_code == 200:\n",
        "                        pdf_path = os.path.join(category_dir, f\"pdfs/{paper_id}.pdf\")\n",
        "                        with open(pdf_path, 'wb') as f:\n",
        "                            f.write(response.content)\n",
        "\n",
        "                        try:\n",
        "                            # Using PyMuPDF for text extraction\n",
        "                            doc = fitz.open(pdf_path)\n",
        "                            full_text = \"\"\n",
        "                            for page in doc:\n",
        "                                full_text += page.get_text()\n",
        "\n",
        "                            full_text_cleaned = clean_text(full_text)\n",
        "                            full_text_cleaned = remove_abstract_from_full_text(full_text_cleaned, abstract)\n",
        "\n",
        "                            if full_text_cleaned:\n",
        "                                txt_path = os.path.join(category_dir, f\"texts/{paper_id}.txt\")\n",
        "                                with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "                                    f.write(full_text_cleaned)\n",
        "                                return full_text_cleaned\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error for paper_id {paper_id}: {e}\")\n",
        "                            return None\n",
        "                    elif response.status_code == 500:\n",
        "                        print(f\"HTTP 500 error for paper_id {paper_id}, retrying...\")\n",
        "                        time.sleep(delay)\n",
        "                    else:\n",
        "                        print(f\"Failed to fetch PDF for paper_id {paper_id}, Status code: {response.status_code}\")\n",
        "                        return None\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Attempt {attempt + 1} to fetch PDF failed: {e}\")\n",
        "                    if attempt < retries - 1:\n",
        "                        time.sleep(delay)\n",
        "                    else:\n",
        "                        print(f\"Failed to fetch PDF for paper_id {paper_id} after {retries} attempts.\")\n",
        "                        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4391fa52",
      "metadata": {
        "id": "4391fa52",
        "outputId": "0b26f6ee-ee13-4481-d402-8ea265b7025c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing category: Networking and Internet Architecture\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2212.03809v2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B3670>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2209.13532v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F1696F0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 1 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_17.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "MuPDF error: syntax error: could not parse color space (102 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (195 0 R)\n",
            "\n",
            "Saved chunk 2 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_18.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2209.11600v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21A890>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 3 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_19.csv\n",
            "Saved chunk 4 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_20.csv\n",
            "Failed to fetch PDF for paper_id 2102.03848v2, Status code: 404\n",
            "Saved chunk 5 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_21.csv\n",
            "Saved chunk 6 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_22.csv\n",
            "Saved chunk 7 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_23.csv\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '597.50787pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '845.04684pt'\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (201 0 R)\n",
            "\n",
            "Saved chunk 8 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_24.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2210.10447v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B0880>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "MuPDF error: syntax error: could not parse color space (383 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (582 0 R)\n",
            "\n",
            "Saved chunk 9 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_25.csv\n",
            "Saved chunk 10 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_26.csv\n",
            "Saved chunk 11 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_27.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2208.12850v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F16A830>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Failed to fetch PDF for paper_id 2208.10569v1, Status code: 429\n",
            "Saved chunk 12 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_28.csv\n",
            "Saved chunk 13 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_29.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2207.11817v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B3310>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 14 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_30.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 15 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_31.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 16 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_32.csv\n",
            "MuPDF error: library error: FT_New_Memory_Face(Times): unknown file format\n",
            "\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 17 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_33.csv\n",
            "Saved chunk 18 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_34.csv\n",
            "Saved chunk 19 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_35.csv\n",
            "Saved chunk 20 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_36.csv\n",
            "Failed to fetch PDF for paper_id 2204.01152v2, Status code: 404\n",
            "Saved chunk 21 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_37.csv\n",
            "MuPDF error: syntax error: could not parse color space (620 0 R)\n",
            "\n",
            "Saved chunk 22 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_38.csv\n",
            "Failed to fetch PDF for paper_id 2204.03504v2, Status code: 404\n",
            "Saved chunk 23 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_39.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2202.07188v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B27A0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 24 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_40.csv\n",
            "Saved chunk 25 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_41.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 26 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_42.csv\n",
            "Saved chunk 27 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_43.csv\n",
            "Failed to fetch PDF for paper_id 2202.02455v2, Status code: 404\n",
            "Saved chunk 28 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_44.csv\n",
            "Saved chunk 29 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_45.csv\n",
            "Saved chunk 30 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_46.csv\n",
            "Saved chunk 31 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_47.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 3 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 4 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 5 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Failed to fetch PDF for paper_id 2111.06263v1 after 5 attempts.\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
            "\n",
            "Saved chunk 32 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_48.csv\n",
            "Saved chunk 33 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_49.csv\n",
            "MuPDF error: syntax error: could not parse color space (456 0 R)\n",
            "\n",
            "Saved chunk 34 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_50.csv\n",
            "Failed to fetch PDF for paper_id 2109.15017v2, Status code: 404\n",
            "HTTP 500 error for paper_id 2109.11607v1, retrying...\n",
            "HTTP 500 error for paper_id 2109.11607v1, retrying...\n",
            "Attempt 3 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTTP 500 error for paper_id 2109.11607v1, retrying...\n",
            "HTTP 500 error for paper_id 2109.11607v1, retrying...\n",
            "Saved chunk 35 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_51.csv\n",
            "Saved chunk 36 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_52.csv\n",
            "Saved chunk 37 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_53.csv\n",
            "Saved chunk 38 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_54.csv\n",
            "Failed to fetch PDF for paper_id 2108.13176v3, Status code: 404\n",
            "Saved chunk 39 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_55.csv\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A1'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A1'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A1'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'A2'\n",
            "\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2107.07604v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F16AAD0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 40 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_56.csv\n",
            "Failed to fetch PDF for paper_id 2106.13896v2, Status code: 404\n",
            "Saved chunk 41 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_57.csv\n",
            "Failed to fetch PDF for paper_id 2106.11825v2, Status code: 404\n",
            "Saved chunk 42 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_58.csv\n",
            "Saved chunk 43 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_59.csv\n",
            "Saved chunk 44 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_60.csv\n",
            "Failed to fetch PDF for paper_id 2105.05751v2, Status code: 404\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2105.00395v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B0250>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '597.50787pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '796.67715pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '170.71652pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '241.84842pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'epdf'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagebox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'cropbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'bbox'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'clip'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '227.62204pt'\n",
            "\n",
            "Saved chunk 45 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_61.csv\n",
            "Saved chunk 46 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_62.csv\n",
            "Saved chunk 47 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_63.csv\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
            "\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 48 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_64.csv\n",
            "MuPDF error: syntax error: could not parse color space (212 0 R)\n",
            "\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 49 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_65.csv\n",
            "Saved chunk 50 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_66.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Failed to fetch PDF for paper_id 2102.01724v3, Status code: 404\n",
            "Saved chunk 51 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_67.csv\n",
            "Saved chunk 52 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_68.csv\n",
            "Saved chunk 53 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_69.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Failed to fetch PDF for paper_id 2012.15545v2, Status code: 404\n",
            "Saved chunk 54 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_70.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Saved chunk 55 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_71.csv\n",
            "Saved chunk 56 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_72.csv\n",
            "Saved chunk 57 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_73.csv\n",
            "Failed to fetch PDF for paper_id 2011.01410v2, Status code: 404\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "MuPDF error: syntax error: cannot find XObject resource 'arial-minus'\n",
            "\n",
            "Saved chunk 58 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_74.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2010.08620v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21B8E0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 59 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_75.csv\n",
            "Saved chunk 60 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_76.csv\n",
            "Failed to fetch PDF for paper_id 2009.05131v2, Status code: 404\n",
            "Saved chunk 61 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_77.csv\n",
            "Saved chunk 62 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_78.csv\n",
            "Saved chunk 63 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_79.csv\n",
            "Saved chunk 64 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_80.csv\n",
            "MuPDF error: syntax error: could not parse color space (134 0 R)\n",
            "\n",
            "Saved chunk 65 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_81.csv\n",
            "Saved chunk 66 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_82.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2006.14413v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F16A830>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 67 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_83.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 68 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_84.csv\n",
            "Saved chunk 69 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_85.csv\n",
            "Saved chunk 70 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_86.csv\n",
            "Failed to fetch PDF for paper_id 2005.00382v2, Status code: 404\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2004.14993v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21ABF0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 71 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_87.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Failed to fetch PDF for paper_id 2004.08885v3, Status code: 404\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2004.06809v2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F169CF0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 72 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_88.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=2004.00472v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B1930>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 73 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_89.csv\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'GS2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'GS2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'GS2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'GS2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'GS2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'GS2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'GS2'\n",
            "\n",
            "MuPDF error: syntax error: cannot find ExtGState resource 'GS2'\n",
            "\n",
            "Saved chunk 74 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_90.csv\n",
            "MuPDF error: syntax error: could not parse color space (108 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (244 0 R)\n",
            "\n",
            "Saved chunk 75 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_91.csv\n",
            "Failed to fetch PDF for paper_id 2002.11375v3, Status code: 404\n",
            "Saved chunk 76 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_92.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 77 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_93.csv\n",
            "Saved chunk 78 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_94.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 79 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_95.csv\n",
            "Saved chunk 80 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_96.csv\n",
            "Saved chunk 81 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_97.csv\n",
            "Failed to fetch PDF for paper_id 1911.09034v3, Status code: 404\n",
            "Saved chunk 82 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_98.csv\n",
            "Saved chunk 83 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_99.csv\n",
            "Saved chunk 84 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_100.csv\n",
            "Failed to fetch PDF for paper_id 1910.06619v3, Status code: 404\n",
            "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'width'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: 'height'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
            "\n",
            "Saved chunk 85 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_101.csv\n",
            "Saved chunk 86 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_102.csv\n",
            "Failed to fetch PDF for paper_id 1909.07585v1, Status code: 404\n",
            "Saved chunk 87 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_103.csv\n",
            "Saved chunk 88 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_104.csv\n",
            "Failed to fetch PDF for paper_id 1908.09042v3, Status code: 404\n",
            "Saved chunk 89 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_105.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1907.11717v2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B1270>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "MuPDF error: syntax error: could not parse color space (883 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (935 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1060 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1137 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1234 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1297 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1401 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1455 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1630 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1710 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1845 0 R)\n",
            "\n",
            "MuPDF error: syntax error: could not parse color space (1896 0 R)\n",
            "\n",
            "Saved chunk 90 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_106.csv\n",
            "Saved chunk 91 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_107.csv\n",
            "Saved chunk 92 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_108.csv\n",
            "MuPDF error: syntax error: could not parse color space (112 0 R)\n",
            "\n",
            "Saved chunk 93 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_109.csv\n",
            "Saved chunk 94 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_110.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1906.00743v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F16BAF0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 95 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_111.csv\n",
            "Saved chunk 96 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_112.csv\n",
            "Saved chunk 97 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_113.csv\n",
            "Saved chunk 98 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_114.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1904.00226v2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B19C0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 99 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_115.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 100 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_116.csv\n",
            "Saved chunk 101 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_117.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1902.05029v3 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B33D0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 102 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_118.csv\n",
            "Saved chunk 103 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_119.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1901.10388v2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F168760>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Attempt 2 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 3 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 4 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 5 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Failed to fetch PDF for paper_id 1901.06980v1 after 5 attempts.\n",
            "MuPDF error: syntax error: could not parse color space (388 0 R)\n",
            "\n",
            "Saved chunk 104 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_120.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 2 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 3 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 4 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Attempt 5 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out.\n",
            "Failed to fetch PDF for paper_id 1901.06786v5 after 5 attempts.\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Saved chunk 105 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_121.csv\n",
            "Saved chunk 106 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_122.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1812.08605v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21B7C0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "MuPDF error: syntax error: could not parse color space (92 0 R)\n",
            "\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 107 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_123.csv\n",
            "Saved chunk 108 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_124.csv\n",
            "Failed to fetch PDF for paper_id 1811.10032v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 1811.06345v2, Status code: 404\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1811.06274v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F168BE0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch PDF for paper_id 1811.05829v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 1811.05386v2, Status code: 404\n",
            "Saved chunk 109 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_125.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1810.13164v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B0250>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "MuPDF error: syntax error: could not parse color space (2234 0 R)\n",
            "\n",
            "Saved chunk 110 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_126.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 111 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_127.csv\n",
            "Saved chunk 112 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_128.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Failed to fetch PDF for paper_id 1809.06815v4, Status code: 404\n",
            "Saved chunk 113 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_129.csv\n",
            "Failed to fetch PDF for paper_id 1809.03183v2, Status code: 404\n",
            "Saved chunk 114 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_130.csv\n",
            "Saved chunk 115 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_131.csv\n",
            "MuPDF error: syntax error: could not parse color space (279 0 R)\n",
            "\n",
            "Saved chunk 116 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_132.csv\n",
            "Saved chunk 117 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_133.csv\n",
            "MuPDF error: syntax error: unknown keyword: '0S'\n",
            "\n",
            "MuPDF error: syntax error: unknown keyword: '0S'\n",
            "\n",
            "Saved chunk 118 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_134.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1806.08943v2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B3E20>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Failed to fetch PDF for paper_id 1806.07410v3, Status code: 404\n",
            "Saved chunk 119 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_135.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1806.03213v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21AB00>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 120 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_136.csv\n",
            "Saved chunk 121 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_137.csv\n",
            "Saved chunk 122 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_138.csv\n",
            "MuPDF error: syntax error: could not parse color space (570 0 R)\n",
            "\n",
            "Saved chunk 123 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_139.csv\n",
            "Error for paper_id 1804.06733v2: code=5: too many nested graphics states\n",
            "Saved chunk 124 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_140.csv\n",
            "Saved chunk 125 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_141.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 126 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_142.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 127 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_143.csv\n",
            "Saved chunk 128 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_144.csv\n",
            "Saved chunk 129 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_145.csv\n",
            "Saved chunk 130 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_146.csv\n",
            "Failed to fetch PDF for paper_id 1801.01372v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 1712.10063v2, Status code: 404\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1712.06878v2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B0850>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 131 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_147.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 132 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_148.csv\n",
            "Saved chunk 133 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_149.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 134 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_150.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 135 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_151.csv\n",
            "Saved chunk 136 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_152.csv\n",
            "HTTP 500 error for paper_id 1709.03745v1, retrying...\n",
            "HTTP 500 error for paper_id 1709.03745v1, retrying...\n",
            "HTTP 500 error for paper_id 1709.03745v1, retrying...\n",
            "HTTP 500 error for paper_id 1709.03745v1, retrying...\n",
            "HTTP 500 error for paper_id 1709.03745v1, retrying...\n",
            "Saved chunk 137 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_153.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 138 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_154.csv\n",
            "Saved chunk 139 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_155.csv\n",
            "Failed to fetch metadata for paper_id 9904016v1, Status code: 400\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Failed to fetch metadata for paper_id 0603102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0603101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9904011v1, Status code: 400\n",
            "Saved chunk 140 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_156.csv\n",
            "Failed to fetch PDF for paper_id 1504.06813v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 1504.06812v2, Status code: 404\n",
            "Saved chunk 141 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_157.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Failed to fetch PDF for paper_id 1110.0147v2, Status code: 404\n",
            "Failed to fetch metadata for paper_id 0610133v4, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0607077v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0412017v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0112024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809104v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0603076v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0603075v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0503074v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0104012v1, Status code: 400\n",
            "Saved chunk 142 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_158.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved chunk 143 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_159.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 144 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_160.csv\n",
            "Failed to fetch metadata for paper_id 0612108v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608058v5, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0601101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0506021v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0208023v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0106028v1, Status code: 400\n",
            "Saved chunk 145 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_161.csv\n",
            "HTTP 500 error for paper_id 1609.07629v1, retrying...\n",
            "HTTP 500 error for paper_id 1609.07629v1, retrying...\n",
            "HTTP 500 error for paper_id 1609.07629v1, retrying...\n",
            "HTTP 500 error for paper_id 1609.07629v1, retrying...\n",
            "HTTP 500 error for paper_id 1609.07629v1, retrying...\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 146 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_162.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Failed to fetch PDF for paper_id 1502.01482v2, Status code: 404\n",
            "Saved chunk 147 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_163.csv\n",
            "Failed to fetch PDF for paper_id 1201.1090v2, Status code: 404\n",
            "Saved chunk 148 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_164.csv\n",
            "Failed to fetch PDF for paper_id 0812.0192v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 0811.2596v2, Status code: 404\n",
            "Saved chunk 149 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_165.csv\n",
            "Failed to fetch metadata for paper_id 0701130v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612048v1, Status code: 400\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=0611075v2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F218D90>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Failed to fetch metadata for paper_id 0611075v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611064v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610135v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610134v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608077v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608069v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605030v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411042v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411019v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408009v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408002v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0403030v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0403029v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311049v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311035v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0103016v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0006022v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9808004v1, Status code: 400\n",
            "Failed to fetch PDF for paper_id 1201.4999v2, Status code: 404\n",
            "Saved chunk 150 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_166.csv\n",
            "Failed to fetch metadata for paper_id 0511012v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0508029v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0102011v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0309006v4, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0109028v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504107v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0403036v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0107035v1, Status code: 400\n",
            "Saved chunk 151 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_167.csv\n",
            "Failed to fetch metadata for paper_id 0502061v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0412119v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408020v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311017v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0110065v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609047v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608014v1, Status code: 400\n",
            "Saved chunk 152 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_168.csv\n",
            "Failed to fetch metadata for paper_id 9809030v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809039v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809040v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809041v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809042v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809043v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809045v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809046v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809047v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809048v1, Status code: 400\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=9809052v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21B580>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Failed to fetch metadata for paper_id 9809052v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809053v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809054v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809055v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809056v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809057v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809058v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809059v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809062v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809063v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809065v1, Status code: 400\n",
            "Saved chunk 153 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_169.csv\n",
            "Failed to fetch metadata for paper_id 9809066v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809067v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809068v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809069v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809070v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809071v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809072v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809073v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809074v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809075v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809076v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809077v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809078v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809079v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809080v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809082v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809083v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809084v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809085v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809086v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809087v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809088v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809089v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809090v1, Status code: 400\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch metadata for paper_id 9809091v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809092v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809093v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809094v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809095v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809096v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809097v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809098v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809099v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809100v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809101v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9809102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9810006v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9811027v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9811028v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9901011v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9902014v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9904012v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9904013v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9904014v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9904015v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9905006v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 9910018v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0001005v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0001007v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0002011v1, Status code: 400\n",
            "Saved chunk 154 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_170.csv\n",
            "Failed to fetch metadata for paper_id 0006037v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0008006v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0012019v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0105031v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0109005v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0111008v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0111027v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0201004v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0202008v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0204011v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0205001v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0205058v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0207069v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0208024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0208025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0209004v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0209006v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0209011v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0209012v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0209013v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0301011v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0302017v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0303012v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0303014v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0303020v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0303028v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0303029v4, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0303030v4, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0305045v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0305064v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0306037v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307012v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0307026v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0308036v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0309054v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0311013v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0401010v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0402011v4, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0402026v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0403042v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0405096v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0406019v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408022v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0408042v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0409022v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411013v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411017v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411039v1, Status code: 400\n",
            "Saved chunk 155 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_171.csv\n",
            "Failed to fetch metadata for paper_id 0411040v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411043v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411044v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411053v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411054v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411055v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411057v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411058v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411059v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411060v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411061v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411062v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411070v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411079v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411081v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411082v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411083v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411084v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411085v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411086v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411087v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411088v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411089v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0411090v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0412020v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0412039v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0501027v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0503010v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0503051v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0503090v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504002v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504040v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504073v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0504111v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0506071v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0506099v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0507009v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0507021v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0507046v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0508009v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0508010v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0508016v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0508021v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0508131v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0510006v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0510028v1, Status code: 400\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch metadata for paper_id 0510052v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0510082v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0511031v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0511053v1, Status code: 400\n",
            "Saved chunk 156 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_172.csv\n",
            "Failed to fetch metadata for paper_id 0511059v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0511062v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0511080v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0511101v4, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0511102v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0512011v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0512092v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0512094v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0601015v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0601016v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0602034v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0602094v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0603062v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0604017v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0604053v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0604096v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0604105v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605042v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605052v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605061v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605062v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605080v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605089v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605133v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0605134v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0606002v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0606005v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0606054v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0606076v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0607038v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0607097v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0607137v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608012v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608025v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608026v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608041v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608082v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0608109v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609026v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609068v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609069v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609077v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609086v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609098v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609106v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609149v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609150v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609151v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0609152v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610013v1, Status code: 400\n",
            "Saved chunk 157 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_173.csv\n",
            "Failed to fetch metadata for paper_id 0610024v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610032v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610040v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610044v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610078v3, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610087v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610109v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610147v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0610157v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611016v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611056v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611105v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611149v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0611157v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612034v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612040v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612045v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612066v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612130v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0612135v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0701046v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0701094v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0701133v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0701198v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0702022v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0702110v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0702125v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0702158v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0703014v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0703063v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0703107v2, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0703108v1, Status code: 400\n",
            "Failed to fetch metadata for paper_id 0703139v3, Status code: 400\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 158 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_174.csv\n",
            "Saved chunk 159 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_175.csv\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "Failed to fetch PDF for paper_id 0805.1877v2, Status code: 404\n",
            "Saved chunk 160 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_176.csv\n",
            "Failed to fetch PDF for paper_id 0807.4268v2, Status code: 404\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Failed to fetch PDF for paper_id 0812.0193v2, Status code: 404\n",
            "Saved chunk 161 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_177.csv\n",
            "Saved chunk 162 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_178.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=0906.3771v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B0AC0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "MuPDF error: format error: No default Layer config\n",
            "\n",
            "Failed to fetch PDF for paper_id 0907.5488v4, Status code: 404\n",
            "Saved chunk 163 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_179.csv\n",
            "Saved chunk 164 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_180.csv\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=0912.4115v2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F21B520>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10)\n",
            "Saved chunk 165 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_181.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to fetch PDF for paper_id 1001.3483v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 1002.1169v2, Status code: 404\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1002.1186v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002828F169DB0>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 166 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_182.csv\n",
            "Failed to fetch PDF for paper_id 1002.3989v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 1002.4255v2, Status code: 404\n",
            "Saved chunk 167 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_183.csv\n",
            "Saved chunk 168 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_184.csv\n",
            "Failed to fetch PDF for paper_id 1005.4018v2, Status code: 404\n",
            "Saved chunk 169 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_185.csv\n",
            "Failed to fetch PDF for paper_id 1006.4278v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 1006.5047v2, Status code: 404\n",
            "Saved chunk 170 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_186.csv\n",
            "Failed to fetch PDF for paper_id 1007.4109v3, Status code: 404\n",
            "Saved chunk 171 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_187.csv\n",
            "Failed to fetch PDF for paper_id 1009.2378v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 1009.2574v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 1009.2575v2, Status code: 404\n",
            "Failed to fetch PDF for paper_id 1009.2576v3, Status code: 404\n",
            "MuPDF error: syntax error: could not parse color space (168 0 R)\n",
            "\n",
            "Saved chunk 172 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_188.csv\n",
            "Attempt 1 to fetch PDF failed: HTTPConnectionPool(host='arxiv.org', port=80): Read timed out. (read timeout=20)\n",
            "Failed to fetch PDF for paper_id 1010.0325v2, Status code: 404\n",
            "Attempt 1 failed: HTTPConnectionPool(host='export.arxiv.org', port=80): Max retries exceeded with url: /api/query?id_list=1010.4986v1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000282889B1900>, 'Connection to export.arxiv.org timed out. (connect timeout=10)'))\n",
            "Saved chunk 173 to full\\Networking and Internet Architecture\\chunks/cleaned_texts_chunk_189.csv\n",
            "Finished processing category: Networking and Internet Architecture\n"
          ]
        }
      ],
      "source": [
        "print(f\"Processing category: Networking and Internet Architecture\")\n",
        "fetch_full_texts_by_category(cs_papers, 'Networking and Internet Architecture', chunk_size=50)\n",
        "print(f\"Finished processing category: Networking and Internet Architecture\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4f4bb5c",
      "metadata": {
        "id": "e4f4bb5c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}